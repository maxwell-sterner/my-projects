{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"(8_11_21) (max interval)  - (weights only) T3 - AFP - skip true - EfficientNetB0.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"4856b746493c484c853504e5697fc026":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7c6c2d6d0a034c8f8e68d9a9f118ca97","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ada9775272374a4281148ebd95a666d0","IPY_MODEL_dedf6dc779e84ae4b360abf2120db93a"]}},"7c6c2d6d0a034c8f8e68d9a9f118ca97":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ada9775272374a4281148ebd95a666d0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_439fe857ffe344f785378ac7857d71a1","_dom_classes":[],"description":"Dl Completed...: ","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":0,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9c849b77e1d34bc8b5bf61fc7e1f70c9"}},"dedf6dc779e84ae4b360abf2120db93a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2ed84a95c6b04a389ffd807427413a92","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 0/0 [00:07&lt;?, ? url/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c514fb4429e5418b8f8eb0ba4d14e81c"}},"439fe857ffe344f785378ac7857d71a1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"9c849b77e1d34bc8b5bf61fc7e1f70c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2ed84a95c6b04a389ffd807427413a92":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c514fb4429e5418b8f8eb0ba4d14e81c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"68a482c0dae04e53bdfae5050d761e11":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2f2d572acbe046968a6e7e5ccfd96e3a","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_1d739779bd4047fd95d9a7593e51e4ce","IPY_MODEL_83ec01fa5a2d4eafafec0442464a8bf6"]}},"2f2d572acbe046968a6e7e5ccfd96e3a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1d739779bd4047fd95d9a7593e51e4ce":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_593bc1052a6043689412f4d5f1d5d04e","_dom_classes":[],"description":"Dl Size...: ","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":0,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6c4a82a39f054e21aa04c5ff160b344d"}},"83ec01fa5a2d4eafafec0442464a8bf6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d32d2d3a30724976b8d6f8262114c519","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 0/0 [00:07&lt;?, ? MiB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_55e90d511dc04972af6538ea5d68b167"}},"593bc1052a6043689412f4d5f1d5d04e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"6c4a82a39f054e21aa04c5ff160b344d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d32d2d3a30724976b8d6f8262114c519":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"55e90d511dc04972af6538ea5d68b167":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6ea997e8de494aa29180bfefdf7eb061":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e278ea439435498a944caa875ac3dbd1","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_cf361c395d2a4653a7b84c6358c81f7d","IPY_MODEL_266d792304f4472d8b51c391c105c5da"]}},"e278ea439435498a944caa875ac3dbd1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cf361c395d2a4653a7b84c6358c81f7d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_883dca4b276c4922a4f441954ad93470","_dom_classes":[],"description":"Extraction completed...: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e183873fbcd44eb9a2699a3954a4c527"}},"266d792304f4472d8b51c391c105c5da":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e75654be7ae541b4ba1bcaaf29aa99cd","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/1 [00:07&lt;00:00,  7.25s/ file]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2b97a4ea4cf44e7191185782937c730f"}},"883dca4b276c4922a4f441954ad93470":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e183873fbcd44eb9a2699a3954a4c527":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e75654be7ae541b4ba1bcaaf29aa99cd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2b97a4ea4cf44e7191185782937c730f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cf6d812ff14241d697fd15343882c60f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_157fb82d035b4f3abb436e0cf241ac04","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_bde900d84f284e359b1620f787cb7f3e","IPY_MODEL_1d67d48038284582803d716efd40eeb7"]}},"157fb82d035b4f3abb436e0cf241ac04":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bde900d84f284e359b1620f787cb7f3e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_df5384a0007447429eab7407cd135e22","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e8c7390deb11436d9177a4e88d1415ab"}},"1d67d48038284582803d716efd40eeb7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5d645675f8104e3f98160725b96c08d4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 10000/0 [00:10&lt;00:00, 1054.40 examples/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_76dcfec9200f4f20a2879efa5343371a"}},"df5384a0007447429eab7407cd135e22":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e8c7390deb11436d9177a4e88d1415ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5d645675f8104e3f98160725b96c08d4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"76dcfec9200f4f20a2879efa5343371a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ae7895c28fca4b30ba03e27e420d399e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e181b900f83f4d4c8c3df38dbf48b876","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_6d25a558ab7a4902803ce8f14d29a7f3","IPY_MODEL_faf51c7b9ce144388deca03672243b55"]}},"e181b900f83f4d4c8c3df38dbf48b876":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6d25a558ab7a4902803ce8f14d29a7f3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_8caf7dfa575a4e8abf9ad06e95ef2626","_dom_classes":[],"description":" 99%","_model_name":"FloatProgressModel","bar_style":"danger","max":10000,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":9927,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_776e53b8d7a3419cad3fa3f8ce7de7d2"}},"faf51c7b9ce144388deca03672243b55":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c4ce04e6ba16415193384e6e31552f16","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 9927/10000 [00:04&lt;00:00, 3067.84 examples/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_98554f9bc06646e28301c3c3c6f2fda2"}},"8caf7dfa575a4e8abf9ad06e95ef2626":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"776e53b8d7a3419cad3fa3f8ce7de7d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c4ce04e6ba16415193384e6e31552f16":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"98554f9bc06646e28301c3c3c6f2fda2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"W-S4v65yCNcP","executionInfo":{"status":"ok","timestamp":1628701354116,"user_tz":420,"elapsed":327,"user":{"displayName":"Thomas Yeh","photoUrl":"","userId":"16777654985296421037"}}},"source":["#from google.colab import drive\n","#drive.mount('/drive')"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"ioQ0TZCBtXlt","colab":{"base_uri":"https://localhost:8080/","height":315,"referenced_widgets":["4856b746493c484c853504e5697fc026","7c6c2d6d0a034c8f8e68d9a9f118ca97","ada9775272374a4281148ebd95a666d0","dedf6dc779e84ae4b360abf2120db93a","439fe857ffe344f785378ac7857d71a1","9c849b77e1d34bc8b5bf61fc7e1f70c9","2ed84a95c6b04a389ffd807427413a92","c514fb4429e5418b8f8eb0ba4d14e81c","68a482c0dae04e53bdfae5050d761e11","2f2d572acbe046968a6e7e5ccfd96e3a","1d739779bd4047fd95d9a7593e51e4ce","83ec01fa5a2d4eafafec0442464a8bf6","593bc1052a6043689412f4d5f1d5d04e","6c4a82a39f054e21aa04c5ff160b344d","d32d2d3a30724976b8d6f8262114c519","55e90d511dc04972af6538ea5d68b167","6ea997e8de494aa29180bfefdf7eb061","e278ea439435498a944caa875ac3dbd1","cf361c395d2a4653a7b84c6358c81f7d","266d792304f4472d8b51c391c105c5da","883dca4b276c4922a4f441954ad93470","e183873fbcd44eb9a2699a3954a4c527","e75654be7ae541b4ba1bcaaf29aa99cd","2b97a4ea4cf44e7191185782937c730f","cf6d812ff14241d697fd15343882c60f","157fb82d035b4f3abb436e0cf241ac04","bde900d84f284e359b1620f787cb7f3e","1d67d48038284582803d716efd40eeb7","df5384a0007447429eab7407cd135e22","e8c7390deb11436d9177a4e88d1415ab","5d645675f8104e3f98160725b96c08d4","76dcfec9200f4f20a2879efa5343371a","ae7895c28fca4b30ba03e27e420d399e","e181b900f83f4d4c8c3df38dbf48b876","6d25a558ab7a4902803ce8f14d29a7f3","faf51c7b9ce144388deca03672243b55","8caf7dfa575a4e8abf9ad06e95ef2626","776e53b8d7a3419cad3fa3f8ce7de7d2","c4ce04e6ba16415193384e6e31552f16","98554f9bc06646e28301c3c3c6f2fda2"]},"executionInfo":{"status":"ok","timestamp":1628701379841,"user_tz":420,"elapsed":25224,"user":{"displayName":"Thomas Yeh","photoUrl":"","userId":"16777654985296421037"}},"outputId":"89823d2d-fca9-4964-8d5a-c3c12297957e"},"source":["from tensorflow.keras.applications.efficientnet import EfficientNetB0\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.applications.efficientnet import preprocess_input, decode_predictions\n","import numpy as np\n","import tensorflow_datasets as tfds\n","\n","import tensorflow as tf\n","from tensorflow.keras.layers import Dense, Flatten, Conv2D\n","from tensorflow.keras import Model\n","import matplotlib.pyplot as plt\n","from matplotlib.ticker import PercentFormatter\n","from tensorflow.python.ops import bitwise_ops\n","import random\n","from copy import deepcopy\n","\n","ds = tfds.load('imagenet_v2', split='test', shuffle_files=False)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["\u001b[1mDownloading and preparing dataset imagenet_v2/matched-frequency/1.0.0 (download: 1.17 GiB, generated: 1.16 GiB, total: 2.33 GiB) to /root/tensorflow_datasets/imagenet_v2/matched-frequency/1.0.0...\u001b[0m\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4856b746493c484c853504e5697fc026","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progre…"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"68a482c0dae04e53bdfae5050d761e11","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressSty…"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6ea997e8de494aa29180bfefdf7eb061","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Extraction completed...', max=1.0, styl…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","\n","\n","\n","\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cf6d812ff14241d697fd15343882c60f","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\rShuffling and writing examples to /root/tensorflow_datasets/imagenet_v2/matched-frequency/1.0.0.incompleteG96DQ6/imagenet_v2-test.tfrecord\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ae7895c28fca4b30ba03e27e420d399e","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\u001b[1mDataset imagenet_v2 downloaded and prepared to /root/tensorflow_datasets/imagenet_v2/matched-frequency/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n","\r"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aTr_I7BOcFDn","executionInfo":{"status":"ok","timestamp":1628701414166,"user_tz":420,"elapsed":34336,"user":{"displayName":"Thomas Yeh","photoUrl":"","userId":"16777654985296421037"}}},"source":["# Add denormal filtering codea\n","# Add 0-mantissa code\n","\n","# Custom rounding code to round values inside tensors\n","# Need to apply fix on nearest and stochastic to the custom_round_not_tensor\n","# Or merge into 2 function with options\n","def custom_round(t_value, round_mode = \"truncate\", skip_processing = False, block_round_mode = \"fine_grain_2_contiguous_exponent_reuse\",\n","                 num_exp = 8, num_mantissa = 23, radix_exp = 2, block_size = 128, \n","                 radix_mantissa = 2, is_tensor = True):\n","  '''\n","    is_tensor will toggle between treating t_value as a tensor vs a non-tensor\n","    Assume Float 32 - TODO: check data type and round accordingly\n","    Round all values inside t_value tensor based on the following:\n","    round_mode: truncate, nearest, stochastic, hw_stochastic, custom\n","    num_exp: number of bits for exponent\n","    num_mantissa: number of bits for mantissa\n","    radix_exp: base number for representation of exponent\n","    radix_mantissa: base number for representation of mantissa \n","  '''\n","  MAN_MASK = 0x007FFFFF \n","  EXP_MASK = 0x7F800000\n","  SIGN_MASK = 0x80000000\n","  SIGN_EXP_MASK = 0xFF800000\n","  GUARD_MASK = 0x00000007\n","  GUARD_RIGHT = 0x00000001\n","  GUARD_MID = 0x00000002 \n","  GUARD_LEFT = 0x00000004\n","\n","  # This mask is shifted to the right by reduce_num\n","  # Upper bits ORd with 1's\n","  TRUNCATE_MASK = 0xFF800000\n","\n","  # This will point to the digit after the last digit to keep\n","  NEAREST_PTR = 0x00400000\n","  LOWEST_PTR = 0x00000001\n","\n","  MAN_OVERFLOW = 0x00800000\n","\n","  LARGEST_DENORMAL_EXP_MASK = 0x7F800000\n","  SMALLEST_DENORMAL_EXP_MASK = 0x00000000\n","\n","  IMPLICIT_ONE = 0x00800000\n","\n","  # Number of bits to reduce based on new number of mantissa bits\n","  reduce_num = 23 - num_mantissa\n","\n","  # need to traverse the tensorfine_grain\n","  # convert tensor to numpy array\n","  if skip_processing == True:\n","    length = 1\n","  else:\n","    length = len(t_value)\n","\n","  for t_num in range(length):\n","    if (skip_processing == False):\n","      if is_tensor:\n","        x = t_value[t_num].numpy()\n","      else:\n","        x = t_value[t_num]\n","    else:\n","       x = t_value\n","\n","    temp_convert_x = tf.bitcast(x, tf.int32)\n","    temp_exp_bits = bitwise_ops.bitwise_and(temp_convert_x, EXP_MASK)\n","    temp_exp_bits = bitwise_ops.right_shift(temp_exp_bits,23)\n","    temp_max = tf.math.reduce_max(temp_exp_bits)\n","    temp_min = tf.math.reduce_min(temp_exp_bits)\n","    temp_range = temp_max - temp_min\n","    #print(\"local exp range: \", temp_range)\n","\n","    #print(\"#############################\")\n","    #print(\"This is the original tensor: \", x)\n","\n","    if (round_mode == \"truncate\"):\n","      # Code to truncate the mantissa to the new number of bits\n","      mask = int(TRUNCATE_MASK)\n","      ##print(\"This is the original mask: \",hex(mask))\n","      for j in range(0, num_mantissa):\n","        mask = mask >> 1\n","        mask = mask | 0x80000000\n","      #mask = mask >> reduce_num\n","      #print(\"This is the new mask: \",hex(mask))\n","      # Need to cast float into integer format in order to apply bitwise operations\n","      # then convert back\n","      convert_x = tf.bitcast(x, tf.int32)\n","      convert_x = bitwise_ops.bitwise_and(convert_x, mask)\n","      x = tf.bitcast(convert_x, tf.float32)\n","\n","      if is_tensor:\n","        t_value[t_num] = tf.constant(x)\n","      else:\n","        t_value[t_num] = x\n","\n","      #print(\"This is the new tensor: \", t_value[t_num].numpy())\n","      #print(\"#############################\") \n","      \n","    elif (round_mode == \"afp_sector_update_jamming\"):\n","\n","      # Use the same exponent per tensor\n","      # Truncate bits that are shifted off - TODO: Look into rounding\n","\n","      convert_x = tf.bitcast(x, tf.int32)\n","\n","      sign_bit = bitwise_ops.bitwise_and(convert_x, SIGN_MASK)\n","      exp_bits = bitwise_ops.bitwise_and(convert_x, EXP_MASK)\n","      man_bits = bitwise_ops.bitwise_and(convert_x, MAN_MASK)\n","      save_man_bits = man_bits\n","\n","      exp_bits = bitwise_ops.right_shift(exp_bits,23)\n","      saved_exp = exp_bits\n","\n","      max_exp = tf.math.reduce_max(exp_bits)\n","      shifted_max_exp = bitwise_ops.left_shift(max_exp, 23)\n","\n","      max_tensor = tf.ones_like(exp_bits)\n","      max_tensor = tf.math.multiply(max_exp, max_tensor)\n","      diff_tensor = tf.math.subtract(max_tensor,exp_bits)\n","\n","      ####################\n","      # Sector Update Code\n","      # New code to profile intervals\n","      # print(\"shifted_man_bits: \", save_man_bits)\n","      interval_num = []\n","\n","      for i in range(8):\n","        interval_num.append(0)\n","\n","      shifted_man_bits = bitwise_ops.right_shift(save_man_bits, 1)\n","      shifted_man_bits = bitwise_ops.bitwise_or(shifted_man_bits, NEAREST_PTR)\n","      shifted_man_bits = bitwise_ops.right_shift(shifted_man_bits, diff_tensor)\n","      # implicit_one = tf.math.equal(exp_bits, max_exp) \n","      # implicit_mask = tf.where(implicit_one, NEAREST_PTR, 0)\n","      # shifted_man_bits = bitwise_ops.bitwise_or(shifted_man_bits, implicit_mask)\n","      truncate_mask2 = NEAREST_PTR \n","      # for i in range(num_mantissa - 1):\n","      for i in range(2):\n","        truncate_mask2 = truncate_mask2 >> 1 | NEAREST_PTR\n","\n","      #print(\"truncate_mask2: \", truncate_mask2)\n","      #print(\"shifted_man_bits: \", shifted_man_bits)\n","      interval_bits = bitwise_ops.bitwise_and(shifted_man_bits, truncate_mask2)\n","      #print(\"masked shift_man interval_bits:\")\n","      #print(interval_bits)\n","      interval_bits = bitwise_ops.right_shift(interval_bits, 20)\n","      interval_int = tf.bitcast(interval_bits, tf.int32)\n","      #print(\"interval_bits:\")\n","      #print(interval_bits)\n","      #print(\"interval_int: \", interval_bits)\n","\n","      one_hot_interval = tf.one_hot(interval_int, 8)\n","      #print(\"one_hot_interval:\")\n","      #print(one_hot_interval)\n","\n","      for i in range(8):\n","        for j in range(len(one_hot_interval)):\n","          interval_num[i] += one_hot_interval[j].numpy()\n","\n","      #####\n","      interval_bool = deepcopy(interval_num[0])\n","      #print(\"interval_count: \", interval_bool)\n","      #interval_bool = deepcopy(interval_num)\n","      free_encodings = 0\n","      for i in range(8):\n","        if interval_bool[i] > 0:\n","          interval_bool[i] = 1\n","        else:\n","          free_encodings += 1\n","      #print(\"Free Encodings: \", free_encodings)\n","\n","      ##############\n","      # All same sign update\n","      min_value = tf.math.reduce_min(x)\n","      max_value = tf.math.reduce_max(x)\n","      all_positive = min_value >= 0\n","      all_negative = max_value <= 0\n","      all_one_sign = all_positive or all_negative\n","\n","      ##################\n","      # range code\n","      abs_x = tf.math.abs(x)\n","      max_abs_value = tf.math.reduce_max(abs_x)\n","      min_abs_value = tf.math.reduce_min(abs_x)\n","\n","      # Code to check if min value in block is > 0001 assuming 3-bit mantissa\n","      min_over_half = max_exp - 3\n","      min_over_half = bitwise_ops.left_shift(min_over_half, 23)\n","      min_over_half = tf.bitcast(min_over_half, tf.float32)\n","      min_just_over_half = min_abs_value > min_over_half\n","      # Code to check for round down to 2^max_exp\n","      \n","      range_mask1 = int(NEAREST_PTR) >> 2\n","      # use num_mantissa - 2 to find LSB, use num_mantissa - 1 to find bit to right of LSB to check for rounding down\n","      #for j in range(0, num_mantissa-1):\n","      #  range_mask1 = range_mask1 >> 1\n","      \n","      # truncate max_exp - instead of checking for 1001 (for 3 bit mantissa)\n","      # check for 100\n","      # over_half = shifted_max_exp # bitwise_ops.left_shift(max_exp, 23)\n","\n","      over_half = bitwise_ops.bitwise_or(shifted_max_exp, range_mask1)\n","\n","      just_over_half = tf.bitcast(over_half, tf.float32)\n","\n","      high_exp = max_exp\n","      if (high_exp != 0):\n","        high_exp -= 127\n","\n","      round_down_to_half = max_abs_value < just_over_half\n","      round_down_to_one_inc = False\n","      round_down_to_two_inc = False\n","\n","      extra_num_exp = 0\n","      max_value_float = 0.0\n","      max_mantissa = 0\n","\n","      # If there are gaps in the double precision\n","      # high threshold describes the upper bound for double precision\n","      high_threshold = 0.0\n","      low_threshold = 0.0\n","      min_mantissa = 0\n","      high_precision_gap = False\n","\n","      if (all_one_sign == True):\n","        # num_mantissa += 1\n","        free_encodings += 8\n","\n","      original_num = num_mantissa\n","      #print(\"New block - remaining free encodings: \", free_encodings)\n","      # Code to check for 2x precision - equivalent to 1 extra bit\n","      if (free_encodings > 0):\n","        if all_one_sign == False:\n","          required_encodings = 8 - free_encodings\n","        else:\n","          required_encodings = 16 - free_encodings\n","\n","        while (free_encodings >= required_encodings):\n","          num_mantissa += 1\n","          free_encodings -= required_encodings\n","          required_encodings = 2*required_encodings\n","\n","      #print(\"num_mantissa: (original), (new) \", original_num, num_mantissa)\n","      #print(\"remaining free encodings: \", free_encodings)\n","      # check for remaining free encodings\n","\n","      if (free_encodings > 0):\n","        int_num = 0\n","        # Need to add code here to selectively increase accuracy for sectors\n","        # interval_bool = 2 for sections with higher accuracy\n","        while free_encodings > 0:\n","          if interval_bool[int_num] >= 1:\n","            interval_bool[int_num] += 1\n","            free_encodings -= 1\n","          int_num = (int_num + 1) % 8\n","\n","      ############################\n","      # Create tensor with flags specific to each value to indicate increase in accuracy\n","      # Flag to indicate select range for higher precision\n","      # high_accuracy_sector stores the number of extra bits of precision for the sector\n","      high_accuracy_sector = tf.zeros_like(convert_x)\n","      for i in range(8):\n","        temp_bool = tf.math.equal(interval_int, i)\n","        high_accuracy_sector = tf.where(temp_bool, interval_bool[i] -1 , high_accuracy_sector)\n","\n","      negative_sector = tf.math.less(high_accuracy_sector, 0)\n","      high_accuracy_sector = tf.where(negative_sector, 0, high_accuracy_sector)\n","      #print(\"high_accuracy_sector: \", high_accuracy_sector)\n","      ############################\n","\n","      #print(\"after while loop\")\n","      if (interval_bool[0] > 0):\n","        zero_threshold = num_mantissa - 1 + interval_bool[0] - 1\n","      else:\n","        zero_threshold = num_mantissa - 1  \n","      shift_to_zero = tf.math.greater(diff_tensor, zero_threshold) \n","\n","      # Based on the difference of the exponent, round the bits shifted - start with truncation\n","      exp_bits = bitwise_ops.left_shift(exp_bits,23)\n","      man_bits = bitwise_ops.right_shift(man_bits, diff_tensor)\n","\n","      # Need to mask out the bits lower than the num_mantissa bits\n","      mask = int(TRUNCATE_MASK)\n","      for j in range(0, num_mantissa-1):\n","        mask = mask >> 1\n","        mask = mask | 0x80000000\n","\n","      mask1 = int(TRUNCATE_MASK)\n","      for j in range(0, num_mantissa):\n","        mask1 = mask1 >> 1\n","        mask1 = mask1 | 0x80000000\n","\n","      mask2 = int(TRUNCATE_MASK)\n","      for j in range(0, num_mantissa + 1):\n","        mask2 = mask2 >> 1\n","        mask2 = mask2 | 0x80000000\n","      \n","      mask3 = int(TRUNCATE_MASK)\n","      for j in range(0, num_mantissa + 2):\n","        mask3 = mask3 >> 1\n","        mask3 = mask3 | 0x80000000      \n","      ###########################\n","      # select high precision code\n","      tensor_mask = tf.ones_like(convert_x)\n","      tensor_mask = tf.math.multiply(tensor_mask, mask)\n","      equal_1 = tf.equal(high_accuracy_sector, 1)\n","      tensor_mask = tf.where(equal_1, mask1, tensor_mask)\n","      equal_2 = tf.equal(high_accuracy_sector, 2)\n","      tensor_mask = tf.where(equal_2, mask2, tensor_mask)\n","      equal_3 = tf.equal(high_accuracy_sector, 3)\n","      tensor_mask = tf.where(equal_3, mask3, tensor_mask)   \n","      #print(\"mask, 1, 2, 3: \", mask, mask1, mask2, mask3)\n","      #print(\"equal_1, 2, 3: \", equal_1, equal_2, equal_3)\n","      #print(\"tensor_mask: \", tensor_mask)\n","\n","      ######### Round Code ###########\n","      ## Adding jamming code\n","      ## setting mask with 3 on bits\n","      guard_mask_save = NEAREST_PTR\n","      for i in range(2):\n","        guard_mask_save = guard_mask_save >> 1 |NEAREST_PTR\n","        \n","      for j in range(num_mantissa - 1):\n","        guard_mask = guard_mask_save >> 1\n","\n","      guard_mask1 = guard_mask_save\n","      for j in range(num_mantissa):\n","        guard_mask1 = guard_mask1 >> 1\n","\n","      guard_mask2 = guard_mask_save\n","      for j in range(num_mantissa + 1):\n","        guard_mask2 = guard_mask2 >> 1\n","\n","      guard_mask3 = guard_mask_save\n","      for j in range(num_mantissa + 2):\n","        guard_mask3 = guard_mask3 >> 1\n","      ###########################\n","      # select high precision code\n","      tensor_guard_mask = tf.ones_like(convert_x)\n","      tensor_guard_mask = tf.math.multiply(tensor_mask, guard_mask)\n","      #equal_1 = tf.equal(high_accuracy_sector, 1)\n","      tensor_guard_mask = tf.where(equal_1, guard_mask1, tensor_guard_mask)\n","      #equal_2 = tf.equal(high_accuracy_sector, 2)\n","      tensor_guard_mask = tf.where(equal_2, guard_mask2, tensor_guard_mask)\n","      #equal_3 = tf.equal(high_accuracy_sector, 3)\n","      tensor_guard_mask = tf.where(equal_3, guard_mask3, tensor_guard_mask)  \n","\n","      #print(\"tensor_guard_mask: \", tensor_guard_mask)\n","\n","      ##########################\n","      # Create a 1 in the lsb of each value (lsb_mask)\n","      # Use this as a mask to get the actual lsb,\n","      # zero out this mask based on the condition (actual lsb is 0 and guard bits are all 0)\n","\n","      lsb_mask = NEAREST_PTR << 1\n","      for j in range(num_mantissa - 1):\n","        lsb_mask = lsb_mask >> 1\n","\n","      lsb_mask1 = NEAREST_PTR << 1\n","      for j in range(num_mantissa):\n","        lsb_mask1 = lsb_mask1 >> 1\n","\n","      lsb_mask2 = NEAREST_PTR << 1\n","      for j in range(num_mantissa + 1):\n","        lsb_mask2 = lsb_mask2 >> 1\n","\n","      lsb_mask3 = NEAREST_PTR << 1\n","      for j in range(num_mantissa + 2):\n","        lsb_mask3 = lsb_mask3 >> 1\n","\n","      tensor_lsb_mask = tf.ones_like(convert_x)\n","      tensor_lsb_mask = tf.math.multiply(tensor_lsb_mask, lsb_mask)\n","      #equal_1 = tf.equal(high_accuracy_sector, 1)\n","      tensor_lsb_mask = tf.where(equal_1, lsb_mask1, tensor_lsb_mask)\n","      #equal_2 = tf.equal(high_accuracy_sector, 2)\n","      tensor_lsb_mask = tf.where(equal_2, lsb_mask2, tensor_lsb_mask)\n","      #equal_3 = tf.equal(high_accuracy_sector, 3)\n","      tensor_lsb_mask = tf.where(equal_3, lsb_mask3, tensor_lsb_mask)  \n","\n","      #print(\"tensor_lsb_mask: \", tensor_lsb_mask)\n","      tensor_lsb_bit = bitwise_ops.bitwise_and(man_bits, tensor_lsb_mask)\n","      lsb_bit = tf.cast(tensor_lsb_bit, tf.bool)\n","      #print(\"lsb_bit: \", lsb_bit)\n","      \n","      #print(\"guard_mask \", guard_mask)\n","      #print(\"tensor_guard_mask: \", tensor_guard_mask)\n","      ######### Round Code ###########\n","\n","      #print(\"guard_mask: \", guard_mask)\n","      # guard_bit = bitwise_ops.bitwise_and(man_bits, guard_mask)\n","      #print(\"man_bits: \", man_bits)\n","      guard_bit = bitwise_ops.bitwise_and(man_bits, tensor_guard_mask)\n","      #print(\"3 guard_bits: \", guard_bit)\n","      ######### Jamming Code\n","      # guard_bit is true if any of guard bits is a 1 -  jamming\n","      guard_bit = tf.cast(guard_bit, tf.bool)\n","      jamming_lsb = tf.logical_or(lsb_bit, guard_bit)\n","      jamming_mantissa = bitwise_ops.bitwise_or(man_bits, tensor_lsb_mask)\n","      #print(\"jamming_lsb, mantissa: \", jamming_lsb, jamming_mantissa)\n","      man_bits = tf.where(jamming_lsb, jamming_mantissa, man_bits)\n","\n","      not_shift_zero = tf.math.less_equal(diff_tensor, zero_threshold)\n","      guard_bit = tf.logical_and(guard_bit, not_shift_zero)\n","\n","      #print(\"guard_bit: \", guard_bit)\n","\n","      #### Need to work on this code to enable rounding!!!\n","      # final_round_exp = tf.math.subtract(saved_exp, num_mantissa - 1)\n","      '''\n","      final_round_exp = tf.math.subtract(saved_exp, num_mantissa - 1)\n","      final_round_exp = tf.math.subtract(final_round_exp, high_accuracy_sector)\n","      final_round_exp = tf.math.add(final_round_exp, diff_tensor) \n","      final_round_exp = bitwise_ops.left_shift(final_round_exp,23)\n","      final_round_value = bitwise_ops.bitwise_or(sign_bit, final_round_exp)\n","      final_round_float = tf.bitcast(final_round_value, tf.float32)\n","      '''\n","      #print(\"final_round_float: \", final_round_float)\n","      \n","      ###############################\n","\n","      # man_bits = bitwise_ops.bitwise_and(man_bits, mask)\n","      man_bits = bitwise_ops.bitwise_and(man_bits, tensor_mask)\n","      man_bits = bitwise_ops.left_shift(man_bits, diff_tensor)\n","\n","      final_value = bitwise_ops.bitwise_or(sign_bit,exp_bits)\n","      final_value = bitwise_ops.bitwise_or(final_value, man_bits)\n","\n","      zeros_tensor = tf.zeros_like(man_bits)\n","      \n","      exp_only = tf.math.greater(diff_tensor, num_mantissa - 1)\n","      not_shift_zero = tf.math.less_equal(diff_tensor, zero_threshold)\n","      exp_only = tf.logical_and(exp_only, not_shift_zero)\n","      sign_exp_bits = bitwise_ops.bitwise_and(convert_x, SIGN_EXP_MASK)\n","      zeros_tensor = tf.where(exp_only, sign_exp_bits , zeros_tensor  )\n","\n","      final_value = tf.where(shift_to_zero, zeros_tensor, final_value)\n","\n","      #final_value = bitwise_ops.bitwise_and(final_value, mask)\n","\n","      x = tf.bitcast(final_value, tf.float32)\n","\n","      # x = tf.where(guard_bit, x + final_round_float, x )\n","\n","      if skip_processing == False:\n","        if is_tensor:\n","          t_value[t_num] = tf.constant(x)\n","        else:\n","          t_value[t_num] = x\n","      else:\n","        t_value = x\n","\n","      \n","    elif (round_mode == \"afp_sector_update_jamming_zero_new\"):\n","\n","      # Use the same exponent per tensor\n","      # Truncate bits that are shifted off - TODO: Look into rounding\n","\n","      convert_x = tf.bitcast(x, tf.int32)\n","\n","      sign_bit = bitwise_ops.bitwise_and(convert_x, SIGN_MASK)\n","      exp_bits = bitwise_ops.bitwise_and(convert_x, EXP_MASK)\n","      man_bits = bitwise_ops.bitwise_and(convert_x, MAN_MASK)\n","      save_man_bits = man_bits\n","\n","      exp_bits = bitwise_ops.right_shift(exp_bits,23)\n","      saved_exp = exp_bits\n","\n","      max_exp = tf.math.reduce_max(exp_bits)\n","      shifted_max_exp = bitwise_ops.left_shift(max_exp, 23)\n","\n","      max_tensor = tf.ones_like(exp_bits)\n","      max_tensor = tf.math.multiply(max_exp, max_tensor)\n","      #print(\"(check) max_tensor: \", max_tensor)\n","      diff_tensor = tf.math.subtract(max_tensor,exp_bits)\n","\n","      ####################\n","      # Sector Update Code\n","      # New code to profile intervals\n","      # print(\"shifted_man_bits: \", save_man_bits)\n","      interval_num = []\n","\n","      for i in range(2**num_mantissa):\n","        interval_num.append(0)\n","\n","      shifted_man_bits = bitwise_ops.right_shift(save_man_bits, 1)\n","      shifted_man_bits = bitwise_ops.bitwise_or(shifted_man_bits, NEAREST_PTR)\n","      shifted_man_bits = bitwise_ops.right_shift(shifted_man_bits, diff_tensor)\n","      # implicit_one = tf.math.equal(exp_bits, max_exp) \n","      # implicit_mask = tf.where(implicit_one, NEAREST_PTR, 0)\n","      # shifted_man_bits = bitwise_ops.bitwise_or(shifted_man_bits, implicit_mask)\n","      truncate_mask2 = NEAREST_PTR \n","      # for i in range(num_mantissa - 1):\n","      for i in range(num_mantissa -1):\n","        truncate_mask2 = truncate_mask2 >> 1 | NEAREST_PTR\n","\n","      # Stopped here - adding as many intervals as quantization levels\n","\n","      #print(\"truncate_mask2: \", truncate_mask2)\n","      #print(\"shifted_man_bits: \", shifted_man_bits)\n","      interval_bits = bitwise_ops.bitwise_and(shifted_man_bits, truncate_mask2)\n","      #print(\"masked shift_man interval_bits:\")\n","      #print(interval_bits)\n","      interval_bits = bitwise_ops.right_shift(interval_bits, 23 - num_mantissa)\n","      interval_int = tf.bitcast(interval_bits, tf.int32)\n","      #print(\"interval_bits:\")\n","      #print(interval_bits)\n","      #print(\"interval_int: \", interval_bits)\n","      #print(\"x: \", x)\n","      one_hot_interval = tf.one_hot(interval_int, 2**num_mantissa)\n","      #print(\"one_hot_interval:\")\n","      #print(one_hot_interval)\n","\n","      for i in range(2**num_mantissa):\n","        for j in range(len(one_hot_interval)):\n","          interval_num[i] += one_hot_interval[j].numpy()\n","\n","      #####\n","      interval_bool = deepcopy(interval_num[0])\n","      interval_num_saved = deepcopy(interval_num[0])\n","      #print(\"interval_count: \", interval_bool)\n","      #interval_bool = deepcopy(interval_num)\n","      if (len(interval_bool) == 1):\n","        interval_bool = interval_bool[0]\n","      #print(\"interval_bool: \", interval_bool)\n","      free_encodings = 0\n","      for i in range(2**num_mantissa):\n","        if interval_bool[i] > 0:\n","          interval_bool[i] = 1\n","        else:\n","          free_encodings += 1\n","      #print(\"Free Encodings: \", free_encodings)\n","\n","      ##############\n","      # All same sign update\n","      min_value = tf.math.reduce_min(x)\n","      max_value = tf.math.reduce_max(x)\n","      all_positive = min_value >= 0\n","      all_negative = max_value <= 0\n","      all_one_sign = all_positive or all_negative\n","\n","      ##################\n","      # range code\n","      abs_x = tf.math.abs(x)\n","      max_abs_value = tf.math.reduce_max(abs_x)\n","      min_abs_value = tf.math.reduce_min(abs_x)\n","\n","      # Code to check if min value in block is > 0001 assuming 3-bit mantissa\n","      min_over_half = max_exp - 3\n","      min_over_half = bitwise_ops.left_shift(min_over_half, 23)\n","      min_over_half = tf.bitcast(min_over_half, tf.float32)\n","      min_just_over_half = min_abs_value > min_over_half\n","      # Code to check for round down to 2^max_exp\n","      \n","      range_mask1 = int(NEAREST_PTR) >> 2\n","      # use num_mantissa - 2 to find LSB, use num_mantissa - 1 to find bit to right of LSB to check for rounding down\n","      #for j in range(0, num_mantissa-1):\n","      #  range_mask1 = range_mask1 >> 1\n","      \n","      # truncate max_exp - instead of checking for 1001 (for 3 bit mantissa)\n","      # check for 100\n","      # over_half = shifted_max_exp # bitwise_ops.left_shift(max_exp, 23)\n","\n","      over_half = bitwise_ops.bitwise_or(shifted_max_exp, range_mask1)\n","\n","      just_over_half = tf.bitcast(over_half, tf.float32)\n","\n","      high_exp = max_exp\n","      if (high_exp != 0):\n","        high_exp -= 127\n","\n","      round_down_to_half = max_abs_value < just_over_half\n","      round_down_to_one_inc = False\n","      round_down_to_two_inc = False\n","\n","      extra_num_exp = 0\n","      max_value_float = 0.0\n","      max_mantissa = 0\n","\n","      # If there are gaps in the double precision\n","      # high threshold describes the upper bound for double precision\n","      high_threshold = 0.0\n","      low_threshold = 0.0\n","      min_mantissa = 0\n","      high_precision_gap = False\n","\n","      if (all_one_sign == True):\n","        # num_mantissa += 1\n","        free_encodings += 2**num_mantissa\n","\n","      original_num = num_mantissa\n","      #print(\"New block - remaining free encodings: \", free_encodings)\n","      # Code to check for 2x precision - equivalent to 1 extra bit\n","      if (free_encodings > 0):\n","        if all_one_sign == False:\n","          required_encodings = (2**num_mantissa) - free_encodings\n","        else:\n","          required_encodings = 2*(2**num_mantissa) - free_encodings\n","\n","        while (free_encodings >= required_encodings):\n","          num_mantissa += 1\n","          free_encodings -= required_encodings\n","          required_encodings = 2*required_encodings\n","\n","      #print(\"num_mantissa: (original), (new), free encodings  \", original_num, num_mantissa, free_encodings)\n","      #if (num_mantissa - original_num > 3):\n","      #  print(\"x: \", x)\n","      #  print(\"interval_bool: \", interval_bool)\n","      #print(\"remaining free encodings: \", free_encodings)\n","      # check for remaining free encodings\n","\n","      if (free_encodings > 0):\n","        int_num = 0\n","        # Need to add code here to selectively increase accuracy for sectors\n","        # interval_bool = 2 for sections with higher accuracy\n","        while free_encodings > 0:\n","          if interval_bool[int_num] >= 1:\n","            interval_bool[int_num] += 1\n","            free_encodings -= 1\n","          int_num = (int_num + 1) % (2**original_num)\n","\n","      ############################\n","      # Create tensor with flags specific to each value to indicate increase in accuracy\n","      # Flag to indicate select range for higher precision\n","      # high_accuracy_sector stores the number of extra bits of precision for the sector\n","      high_accuracy_sector = tf.zeros_like(convert_x)\n","      for i in range(2**original_num):\n","        temp_bool = tf.math.equal(interval_int, i)\n","        high_accuracy_sector = tf.where(temp_bool, interval_bool[i] -1 , high_accuracy_sector)\n","\n","      negative_sector = tf.math.less(high_accuracy_sector, 0)\n","      high_accuracy_sector = tf.where(negative_sector, 0, high_accuracy_sector)\n","      #print(\"interval_bool: \", interval_bool)\n","      #print(\"high_accuracy_sector: \", high_accuracy_sector)\n","      ############################\n","\n","\n","      # 8_8_21 - Stopped here - for increased intervals change !!!\n","\n","      #print(\"after while loop\")\n","      if (interval_bool[0] > 0):\n","        zero_threshold = num_mantissa - 1 + interval_bool[0] - 1\n","      else:\n","        zero_threshold = num_mantissa - 1  \n","      shift_to_zero = tf.math.greater(diff_tensor, zero_threshold)\n","\n","      #non_zero_mantissa = tf.math.not_equal(x, 0.0) \n","      non_zero_mantissa = tf.math.less_equal(diff_tensor, zero_threshold + 3)\n","      non_zero_mantissa = tf.math.logical_and(shift_to_zero, non_zero_mantissa)\n","\n","      # Based on the difference of the exponent, round the bits shifted - start with truncation\n","      exp_bits = bitwise_ops.left_shift(exp_bits,23)\n","      man_bits = bitwise_ops.right_shift(man_bits, diff_tensor)\n","\n","      # Need to mask out the bits lower than the num_mantissa bits\n","      mask = int(TRUNCATE_MASK)\n","      for j in range(0, num_mantissa-1):\n","        mask = mask >> 1\n","        mask = mask | 0x80000000\n","\n","      mask1 = (mask >> 1) | 0x80000000\n","      mask2 = (mask1 >> 1) | 0x80000000      \n","      mask3 = (mask2 >> 1) | 0x80000000\n","      mask4 = (mask3 >> 1) | 0x80000000 \n","      mask5 = (mask4 >> 1) | 0x80000000\n","      mask6 = (mask5 >> 1) | 0x80000000       \n","      '''\n","      mask1 = int(TRUNCATE_MASK)\n","      for j in range(0, num_mantissa):\n","        mask1 = mask1 >> 1\n","        mask1 = mask1 | 0x80000000\n","\n","      mask2 = int(TRUNCATE_MASK)\n","      for j in range(0, num_mantissa + 1):\n","        mask2 = mask2 >> 1\n","        mask2 = mask2 | 0x80000000\n","      \n","      mask3 = int(TRUNCATE_MASK)\n","      for j in range(0, num_mantissa + 2):\n","        mask3 = mask3 >> 1\n","        mask3 = mask3 | 0x80000000\n","\n","      mask4 = int(TRUNCATE_MASK)\n","      for j in range(0, num_mantissa + 3):\n","        mask4 = mask4 >> 1\n","        mask4 = mask4 | 0x80000000\n","\n","      mask5 = int(TRUNCATE_MASK)\n","      for j in range(0, num_mantissa + 4):\n","        mask5 = mask5 >> 1\n","        mask5 = mask5 | 0x80000000\n","      \n","      mask6 = int(TRUNCATE_MASK)\n","      for j in range(0, num_mantissa + 5):\n","        mask6 = mask6 >> 1\n","        mask6 = mask6 | 0x80000000  \n","      '''\n","\n","      ###########################\n","      # select high precision code\n","      tensor_mask = tf.ones_like(convert_x)\n","      tensor_mask = tf.math.multiply(tensor_mask, mask)\n","      equal_1 = tf.equal(high_accuracy_sector, 1)\n","      equal_2 = tf.equal(high_accuracy_sector, 2)\n","      equal_3 = tf.equal(high_accuracy_sector, 3)\n","      equal_4 = tf.equal(high_accuracy_sector, 4)\n","      equal_5 = tf.equal(high_accuracy_sector, 5)\n","      equal_6 = tf.equal(high_accuracy_sector, 6)\n","\n","      baseline_tensor = bitwise_ops.bitwise_and(man_bits, tensor_mask)\n","\n","      temp_mask = tf.ones_like(convert_x)\n","      temp_mask = tf.math.multiply(temp_mask, mask1)      \n","      temp_tensor = bitwise_ops.bitwise_and(man_bits, temp_mask)\n","      # Found 1 in higher bit\n","      overwrite = tf.math.not_equal(baseline_tensor, temp_tensor) \n","      overwrite_check = tf.math.greater(high_accuracy_sector, 1)\n","      overwrite1 = tf.math.logical_and(overwrite, overwrite_check)\n","\n","      temp_mask = tf.ones_like(convert_x)\n","      temp_mask = tf.math.multiply(temp_mask, mask2)      \n","      temp_tensor2 = bitwise_ops.bitwise_and(man_bits, temp_mask)\n","      # Found 1 in higher bit\n","      overwrite = tf.math.not_equal(temp_tensor, temp_tensor2) \n","      overwrite_check = tf.math.greater(high_accuracy_sector, 3)\n","      overwrite2 = tf.math.logical_and(overwrite, overwrite_check)\n","      \n","      temp_mask = tf.ones_like(convert_x)\n","      temp_mask = tf.math.multiply(temp_mask, mask3)      \n","      temp_tensor3 = bitwise_ops.bitwise_and(man_bits, temp_mask)\n","      # Found 1 in higher bit\n","      overwrite = tf.math.not_equal(temp_tensor2, temp_tensor3) \n","      overwrite_check = tf.math.greater(high_accuracy_sector, 4)\n","      overwrite3 = tf.math.logical_and(overwrite, overwrite_check)\n","\n","      temp_mask = tf.ones_like(convert_x)\n","      temp_mask = tf.math.multiply(temp_mask, mask4)      \n","      temp_tensor4 = bitwise_ops.bitwise_and(man_bits, temp_mask)\n","      # Found 1 in higher bit\n","      overwrite = tf.math.not_equal(temp_tensor3, temp_tensor4) \n","      overwrite_check = tf.math.greater(high_accuracy_sector, 5)\n","      overwrite4 = tf.math.logical_and(overwrite, overwrite_check)\n","\n","      tensor_mask = tf.where(equal_1, mask1, tensor_mask)\n","      tensor_mask = tf.where(equal_2, mask2, tensor_mask)\n","      tensor_mask = tf.where(equal_3, mask3, tensor_mask)\n","      tensor_mask = tf.where(equal_4, mask1, tensor_mask)\n","      tensor_mask = tf.where(equal_5, mask2, tensor_mask)\n","      tensor_mask = tf.where(equal_6, mask3, tensor_mask)     \n","      #print(\"mask, 1, 2, 3: \", mask, mask1, mask2, mask3)\n","      #print(\"equal_1, 2, 3: \", equal_1, equal_2, equal_3)\n","      #print(\"tensor_mask: \", tensor_mask)\n","\n","      tensor_mask = tf.where(overwrite1, mask1, tensor_mask)\n","      tensor_mask = tf.where(overwrite2, mask2, tensor_mask)\n","      tensor_mask = tf.where(overwrite3, mask3, tensor_mask)\n","      tensor_mask = tf.where(overwrite4, mask4, tensor_mask)\n","          \n","      '''\n","      power_of_two_mask1 = NEAREST_PTR\n","      power_of_two_mask2 = power_of_two_mask1 >> 1 | NEAREST_PTR\n","      power_of_two_mask3 = power_of_two_mask2 >> 1 | NEAREST_PTR\n","      power_of_two_mask4 = power_of_two_mask3 >> 1 | NEAREST_PTR\n","      power_of_two_mask5 = power_of_two_mask4 >> 1 | NEAREST_PTR\n","      power_of_two_mask6 = power_of_two_mask5 >> 1 | NEAREST_PTR      \n","\n","      tensor_power_mask = tf.zeros_like(convert_x)\n","      tensor_power_mask = tf.where(equal_1, power_of_two_mask1, tensor_power_mask)\n","      tensor_power_mask = tf.where(equal_2, power_of_two_mask2, tensor_power_mask)\n","      tensor_power_mask = tf.where(equal_3, power_of_two_mask3, tensor_power_mask)\n","      tensor_power_mask = tf.where(equal_4, power_of_two_mask4, tensor_power_mask)\n","      tensor_power_mask = tf.where(equal_5, power_of_two_mask5, tensor_power_mask)\n","      tensor_power_mask = tf.where(equal_6, power_of_two_mask6, tensor_power_mask)\n","\n","      tensor_power_mask = bitwise_ops.right_shift(tensor_power_mask, num_mantissa -1)\n","      '''\n","\n","\n","      ######### Round Code ###########\n","      ## Adding jamming code\n","      ## setting mask with 3 on bits\n","      guard_mask_save = NEAREST_PTR\n","      for i in range(2):\n","        guard_mask_save = (guard_mask_save >> 1) |NEAREST_PTR\n","\n","      #print(\"guard_mask_save: \", guard_mask_save)  \n","      #for j in range(num_mantissa - 1):\n","      #  guard_mask = guard_mask_save >> 1\n","      guard_mask = guard_mask_save >> (num_mantissa -1)\n","      guard_mask1 = guard_mask_save >> num_mantissa\n","      guard_mask2 = guard_mask_save >> (num_mantissa + 1)\n","      guard_mask3 = guard_mask_save >> (num_mantissa + 2)\n","      guard_mask4 = guard_mask_save >> (num_mantissa + 3)\n","      guard_mask5 = guard_mask_save >> (num_mantissa + 4)\n","      guard_mask6 = guard_mask_save >> (num_mantissa + 5)    \n","\n","      # print(\"guard_mask: \", guard_mask)  \n","      # print(\"guard_mask1: \", guard_mask1)  \n","      # print(\"guard_mask2: \", guard_mask2)  \n","      # print(\"guard_mask3: \", guard_mask3) \n","      # print(\"guard_mask4: \", guard_mask4)  \n","      # print(\"guard_mask5: \", guard_mask5) \n","      # print(\"guard_mask6: \", guard_mask6)  \n","\n","                        \n","      '''\n","      for j in range(num_mantissa):\n","        guard_mask1 = guard_mask1 >> 1\n","\n","      guard_mask2 = guard_mask_save\n","      for j in range(num_mantissa + 1):\n","        guard_mask2 = guard_mask2 >> 1\n","\n","      guard_mask3 = guard_mask_save\n","      for j in range(num_mantissa + 2):\n","        guard_mask3 = guard_mask3 >> 1\n","\n","      guard_mask4 = guard_mask_save\n","      for j in range(num_mantissa + 3):\n","        guard_mask4 = guard_mask4 >> 1\n","\n","      guard_mask5 = guard_mask_save\n","      for j in range(num_mantissa + 4):\n","        guard_mask5 = guard_mask5 >> 1\n","\n","      guard_mask6 = guard_mask_save\n","      for j in range(num_mantissa + 5):\n","        guard_mask6 = guard_mask6 >> 1\n","      '''\n","      ###########################\n","      # select high precision code\n","      tensor_guard_mask = tf.ones_like(convert_x)\n","      #tensor_guard_mask = tf.math.multiply(tensor_mask, guard_mask)\n","      tensor_guard_mask = tf.where(True, guard_mask, guard_mask)\n","      #print(\"tensor_guard_mask: \", tensor_guard_mask)\n","      #equal_1 = tf.equal(high_accuracy_sector, 1)\n","      tensor_guard_mask = tf.where(equal_1, guard_mask1, tensor_guard_mask)\n","      #equal_2 = tf.equal(high_accuracy_sector, 2)\n","      tensor_guard_mask = tf.where(equal_2, guard_mask2, tensor_guard_mask)\n","      #equal_3 = tf.equal(high_accuracy_sector, 3)\n","      tensor_guard_mask = tf.where(equal_3, guard_mask3, tensor_guard_mask)  \n","      tensor_guard_mask = tf.where(equal_4, guard_mask4, tensor_guard_mask)\n","      tensor_guard_mask = tf.where(equal_5, guard_mask5, tensor_guard_mask)\n","      tensor_guard_mask = tf.where(equal_6, guard_mask6, tensor_guard_mask)  \n","\n","      #print(\"tensor_guard_mask: \", tensor_guard_mask)\n","\n","      ##########################\n","      # Create a 1 in the lsb of each value (lsb_mask)\n","      # Use this as a mask to get the actual lsb,\n","      # zero out this mask based on the condition (actual lsb is 0 and guard bits are all 0)\n","\n","      lsb_mask = NEAREST_PTR << 1\n","      lsb_mask = lsb_mask >> (num_mantissa -1)\n","      lsb_mask1 = lsb_mask >> 1\n","      lsb_mask2 = lsb_mask1 >> 1\n","      lsb_mask3 = lsb_mask2 >> 1\n","      lsb_mask4 = lsb_mask3 >> 1\n","      lsb_mask5 = lsb_mask4 >> 1 \n","      lsb_mask6 = lsb_mask5 >> 1                       \n","      '''\n","      for j in range(num_mantissa - 1):\n","        lsb_mask = lsb_mask >> 1\n","\n","      lsb_mask1 = NEAREST_PTR << 1\n","      for j in range(num_mantissa):\n","        lsb_mask1 = lsb_mask1 >> 1\n","\n","      lsb_mask2 = NEAREST_PTR << 1\n","      for j in range(num_mantissa + 1):\n","        lsb_mask2 = lsb_mask2 >> 1\n","\n","      lsb_mask3 = NEAREST_PTR << 1\n","      for j in range(num_mantissa + 2):\n","        lsb_mask3 = lsb_mask3 >> 1\n","\n","      lsb_mask4 = NEAREST_PTR << 1\n","      for j in range(num_mantissa + 3):\n","        lsb_mask4 = lsb_mask4 >> 1\n","\n","      lsb_mask5 = NEAREST_PTR << 1\n","      for j in range(num_mantissa + 4):\n","        lsb_mask5 = lsb_mask5 >> 1\n","\n","      lsb_mask6 = NEAREST_PTR << 1\n","      for j in range(num_mantissa + 5):\n","        lsb_mask6 = lsb_mask6 >> 1\n","      '''\n","\n","      tensor_lsb_mask = tf.ones_like(convert_x)\n","      tensor_lsb_mask = tf.math.multiply(tensor_lsb_mask, lsb_mask)\n","      #equal_1 = tf.equal(high_accuracy_sector, 1)\n","      tensor_lsb_mask = tf.where(equal_1, lsb_mask1, tensor_lsb_mask)\n","      #equal_2 = tf.equal(high_accuracy_sector, 2)\n","      tensor_lsb_mask = tf.where(equal_2, lsb_mask2, tensor_lsb_mask)\n","      #equal_3 = tf.equal(high_accuracy_sector, 3)\n","      tensor_lsb_mask = tf.where(equal_3, lsb_mask3, tensor_lsb_mask)  \n","      tensor_lsb_mask = tf.where(equal_4, lsb_mask4, tensor_lsb_mask)\n","      tensor_lsb_mask = tf.where(equal_5, lsb_mask5, tensor_lsb_mask)\n","      tensor_lsb_mask = tf.where(equal_6, lsb_mask6, tensor_lsb_mask)  \n","\n","      #print(\"tensor_lsb_mask: \", tensor_lsb_mask)\n","      tensor_lsb_bit = bitwise_ops.bitwise_and(man_bits, tensor_lsb_mask)\n","      lsb_bit = tf.cast(tensor_lsb_bit, tf.bool)\n","      #print(\"lsb_bit: \", lsb_bit)\n","      \n","      #print(\"guard_mask \", guard_mask)\n","      #print(\"tensor_guard_mask: \", tensor_guard_mask)\n","      ######### Round Code ###########\n","\n","      #print(\"guard_mask: \", guard_mask)\n","      # guard_bit = bitwise_ops.bitwise_and(man_bits, guard_mask)\n","      #print(\"man_bits: \", man_bits)\n","      guard_bit = bitwise_ops.bitwise_and(man_bits, tensor_guard_mask)\n","      #print(\"3 guard_bits: \", guard_bit)\n","      ######### Jamming Code\n","      # guard_bit is true if any of guard bits is a 1 -  jamming\n","      guard_bit = tf.cast(guard_bit, tf.bool)\n","      jamming_lsb = tf.logical_or(lsb_bit, guard_bit)\n","      jamming_mantissa = bitwise_ops.bitwise_or(man_bits, tensor_lsb_mask)\n","      #print(\"jamming_lsb, mantissa: \", jamming_lsb, jamming_mantissa)\n","      man_bits = tf.where(jamming_lsb, jamming_mantissa, man_bits)\n","      #print(\"man_bits: \", man_bits)\n","\n","\n","      not_shift_zero = tf.math.less_equal(diff_tensor, zero_threshold)\n","      guard_bit = tf.logical_and(guard_bit, not_shift_zero)\n","\n","      #print(\"guard_bit: \", guard_bit)\n","\n","      #### Need to work on this code to enable rounding!!!\n","      # final_round_exp = tf.math.subtract(saved_exp, num_mantissa - 1)\n","      '''\n","      final_round_exp = tf.math.subtract(saved_exp, num_mantissa - 1)\n","      final_round_exp = tf.math.subtract(final_round_exp, high_accuracy_sector)\n","      final_round_exp = tf.math.add(final_round_exp, diff_tensor) \n","      final_round_exp = bitwise_ops.left_shift(final_round_exp,23)\n","      final_round_value = bitwise_ops.bitwise_or(sign_bit, final_round_exp)\n","      final_round_float = tf.bitcast(final_round_value, tf.float32)\n","      '''\n","      #print(\"final_round_float: \", final_round_float)\n","      \n","      ###############################\n","\n","      # man_bits = bitwise_ops.bitwise_and(man_bits, mask)\n","      man_bits = bitwise_ops.bitwise_and(man_bits, tensor_mask)\n","      man_bits = bitwise_ops.left_shift(man_bits, diff_tensor)\n","\n","      final_value = bitwise_ops.bitwise_or(sign_bit,exp_bits)\n","      final_value = bitwise_ops.bitwise_or(final_value, man_bits)\n","\n","      zeros_tensor = tf.zeros_like(man_bits)\n","      \n","      exp_only = tf.math.greater(diff_tensor, num_mantissa - 1)\n","      not_shift_zero = tf.math.less_equal(diff_tensor, zero_threshold)\n","      exp_only = tf.logical_and(exp_only, not_shift_zero)\n","\n","      # non zero mantissa code\n","      #exp_only_new = tf.math.logical_and(shift_to_zero, non_zero_mantissa)\n","\n","      sign_exp_bits = bitwise_ops.bitwise_and(convert_x, SIGN_EXP_MASK)\n","\n","      # stochastic exp only value code\n","      # exp = tf.random.uniform() - min = min exp, max is the exp of max - mantissa - extra encodings\n","      #sign_bit = bitwise_ops.bitwise_and(convert_x, SIGN_MASK)\n","      #random_exp_bits = tf.random.uniform(shape=tf.shape(man_bits),maxval=max_exp - zero_threshold, \n","      #                                    minval=max_exp -zero_threshold-8, dtype=tf.int32)\n","      #print(\"random_exp_bits: \", random_exp_bits)\n","      # random_exp_bits = bitwise_ops.left_shift(random_exp_bits,23)\n","      # random_sign_exp_bits = bitwise_ops.bitwise_or(sign_bit, random_exp_bits)\n","\n","      sign_exp_lsb = bitwise_ops.bitwise_and(convert_x, SIGN_MASK)\n","      exp_lsb = max_exp - (num_mantissa - 1) - (interval_bool[0] - 1)\n","      exp_lsb = bitwise_ops.left_shift(exp_lsb, 23)\n","      sign_exp_lsb = bitwise_ops.bitwise_or(sign_exp_lsb, exp_lsb)\n","\n","      zeros_tensor = tf.where(exp_only, sign_exp_bits , zeros_tensor  )\n","      zeros_tensor = tf.where(non_zero_mantissa, sign_exp_lsb, zeros_tensor)\n","\n","      final_value = tf.where(shift_to_zero, zeros_tensor, final_value)\n","\n","\n","      #final_value= tf.where(exp_only_new, random_sign_exp_bits, final_value)\n","\n","      #final_value = bitwise_ops.bitwise_and(final_value, mask)\n","\n","      x = tf.bitcast(final_value, tf.float32)\n","      #print(\"final x: \", x)\n","      # x = tf.where(guard_bit, x + final_round_float, x )\n","\n","      if skip_processing == False:\n","        if is_tensor:\n","          t_value[t_num] = tf.constant(x)\n","        else:\n","          t_value[t_num] = x\n","      else:\n","        t_value = x\n","\n","\n","\n","      #######################################\n","    elif (round_mode == \"afp_sector_update_jamming_1bit\"):\n","\n","      # Use the same exponent per tensor\n","      # Truncate bits that are shifted off - TODO: Look into rounding\n","\n","      convert_x = tf.bitcast(x, tf.int32)\n","\n","      sign_bit = bitwise_ops.bitwise_and(convert_x, SIGN_MASK)\n","      exp_bits = bitwise_ops.bitwise_and(convert_x, EXP_MASK)\n","      man_bits = bitwise_ops.bitwise_and(convert_x, MAN_MASK)\n","      save_man_bits = man_bits\n","\n","      exp_bits = bitwise_ops.right_shift(exp_bits,23)\n","      saved_exp = exp_bits\n","\n","      max_exp = tf.math.reduce_max(exp_bits)\n","      shifted_max_exp = bitwise_ops.left_shift(max_exp, 23)\n","\n","      max_tensor = tf.ones_like(exp_bits)\n","      max_tensor = tf.math.multiply(max_exp, max_tensor)\n","      diff_tensor = tf.math.subtract(max_tensor,exp_bits)\n","\n","      ####################\n","      # Sector Update Code\n","      # New code to profile intervals\n","      # print(\"shifted_man_bits: \", save_man_bits)\n","      interval_num = []\n","\n","      for i in range(8):\n","        interval_num.append(0)\n","\n","      shifted_man_bits = bitwise_ops.right_shift(save_man_bits, 1)\n","      shifted_man_bits = bitwise_ops.bitwise_or(shifted_man_bits, NEAREST_PTR)\n","      shifted_man_bits = bitwise_ops.right_shift(shifted_man_bits, diff_tensor)\n","      # implicit_one = tf.math.equal(exp_bits, max_exp) \n","      # implicit_mask = tf.where(implicit_one, NEAREST_PTR, 0)\n","      # shifted_man_bits = bitwise_ops.bitwise_or(shifted_man_bits, implicit_mask)\n","      truncate_mask2 = NEAREST_PTR \n","      # for i in range(num_mantissa - 1):\n","      for i in range(2):\n","        truncate_mask2 = truncate_mask2 >> 1 | NEAREST_PTR\n","\n","      #print(\"truncate_mask2: \", truncate_mask2)\n","      #print(\"shifted_man_bits: \", shifted_man_bits)\n","      interval_bits = bitwise_ops.bitwise_and(shifted_man_bits, truncate_mask2)\n","      #print(\"masked shift_man interval_bits:\")\n","      #print(interval_bits)\n","      interval_bits = bitwise_ops.right_shift(interval_bits, 20)\n","      interval_int = tf.bitcast(interval_bits, tf.int32)\n","      #print(\"interval_bits:\")\n","      #print(interval_bits)\n","      #print(\"interval_int: \", interval_bits)\n","\n","      one_hot_interval = tf.one_hot(interval_int, 8)\n","      #print(\"one_hot_interval:\")\n","      #print(one_hot_interval)\n","\n","      for i in range(8):\n","        for j in range(len(one_hot_interval)):\n","          interval_num[i] += one_hot_interval[j].numpy()\n","\n","      #####\n","      interval_bool = deepcopy(interval_num[0])\n","      #print(\"interval_count: \", interval_bool)\n","      #interval_bool = deepcopy(interval_num)\n","      free_encodings = 0\n","      for i in range(8):\n","        if interval_bool[i] > 0:\n","          interval_bool[i] = 1\n","        else:\n","          free_encodings += 1\n","      #print(\"Free Encodings: \", free_encodings)\n","\n","      ##############\n","      # All same sign update\n","      min_value = tf.math.reduce_min(x)\n","      max_value = tf.math.reduce_max(x)\n","      all_positive = min_value >= 0\n","      all_negative = max_value <= 0\n","      all_one_sign = all_positive or all_negative\n","\n","      ##################\n","      # range code\n","      abs_x = tf.math.abs(x)\n","      max_abs_value = tf.math.reduce_max(abs_x)\n","      min_abs_value = tf.math.reduce_min(abs_x)\n","\n","      # Code to check if min value in block is > 0001 assuming 3-bit mantissa\n","      min_over_half = max_exp - 3\n","      min_over_half = bitwise_ops.left_shift(min_over_half, 23)\n","      min_over_half = tf.bitcast(min_over_half, tf.float32)\n","      min_just_over_half = min_abs_value > min_over_half\n","      # Code to check for round down to 2^max_exp\n","      \n","      range_mask1 = int(NEAREST_PTR) >> 2\n","      # use num_mantissa - 2 to find LSB, use num_mantissa - 1 to find bit to right of LSB to check for rounding down\n","      #for j in range(0, num_mantissa-1):\n","      #  range_mask1 = range_mask1 >> 1\n","      \n","      # truncate max_exp - instead of checking for 1001 (for 3 bit mantissa)\n","      # check for 100\n","      # over_half = shifted_max_exp # bitwise_ops.left_shift(max_exp, 23)\n","\n","      over_half = bitwise_ops.bitwise_or(shifted_max_exp, range_mask1)\n","\n","      just_over_half = tf.bitcast(over_half, tf.float32)\n","\n","      high_exp = max_exp\n","      if (high_exp != 0):\n","        high_exp -= 127\n","\n","      round_down_to_half = max_abs_value < just_over_half\n","      round_down_to_one_inc = False\n","      round_down_to_two_inc = False\n","\n","      extra_num_exp = 0\n","      max_value_float = 0.0\n","      max_mantissa = 0\n","\n","      # If there are gaps in the double precision\n","      # high threshold describes the upper bound for double precision\n","      high_threshold = 0.0\n","      low_threshold = 0.0\n","      min_mantissa = 0\n","      high_precision_gap = False\n","\n","      if (all_one_sign == True):\n","        # num_mantissa += 1\n","        free_encodings += 8\n","\n","      original_num = num_mantissa\n","      #print(\"New block - remaining free encodings: \", free_encodings)\n","      # Code to check for 2x precision - equivalent to 1 extra bit\n","      if (free_encodings > 0):\n","        if all_one_sign == False:\n","          required_encodings = 8 - free_encodings\n","        else:\n","          required_encodings = 16 - free_encodings\n","\n","        while (free_encodings >= required_encodings):\n","          num_mantissa += 1\n","          free_encodings -= required_encodings\n","          required_encodings = 2*required_encodings\n","\n","      print(\"num_mantissa: (original), (new) \", original_num, num_mantissa)\n","      #print(\"remaining free encodings: \", free_encodings)\n","      # check for remaining free encodings\n","\n","      if (free_encodings > 0):\n","        int_num = 0\n","        # Need to add code here to selectively increase accuracy for sectors\n","        # interval_bool = 2 for sections with higher accuracy\n","        while free_encodings > 0:\n","          if interval_bool[int_num] >= 1:\n","            interval_bool[int_num] += 1\n","            free_encodings -= 1\n","          int_num = (int_num + 1) % 8\n","\n","      ############################\n","      # Create tensor with flags specific to each value to indicate increase in accuracy\n","      # Flag to indicate select range for higher precision\n","      # high_accuracy_sector stores the number of extra bits of precision for the sector\n","      high_accuracy_sector = tf.zeros_like(convert_x)\n","      for i in range(8):\n","        temp_bool = tf.math.equal(interval_int, i)\n","        high_accuracy_sector = tf.where(temp_bool, interval_bool[i] -1 , high_accuracy_sector)\n","\n","      negative_sector = tf.math.less(high_accuracy_sector, 0)\n","      high_accuracy_sector = tf.where(negative_sector, 0, high_accuracy_sector)\n","      print(\"high_accuracy_sector: \", high_accuracy_sector)\n","      ############################\n","\n","      #print(\"after while loop\")\n","      if (interval_bool[0] > 0):\n","        zero_threshold = num_mantissa - 1 + interval_bool[0] - 1\n","      else:\n","        zero_threshold = num_mantissa - 1  \n","      shift_to_zero = tf.math.greater(diff_tensor, zero_threshold) \n","\n","      # Based on the difference of the exponent, round the bits shifted - start with truncation\n","      exp_bits = bitwise_ops.left_shift(exp_bits,23)\n","      man_bits = bitwise_ops.right_shift(man_bits, diff_tensor)\n","\n","      # Need to mask out the bits lower than the num_mantissa bits\n","      mask = int(TRUNCATE_MASK)\n","      for j in range(0, num_mantissa-1):\n","        mask = mask >> 1\n","        mask = mask | 0x80000000\n","\n","      mask1 = int(TRUNCATE_MASK)\n","      for j in range(0, num_mantissa):\n","        mask1 = mask1 >> 1\n","        mask1 = mask1 | 0x80000000\n","\n","      mask2 = int(TRUNCATE_MASK)\n","      for j in range(0, num_mantissa + 1):\n","        mask2 = mask2 >> 1\n","        mask2 = mask2 | 0x80000000\n","      \n","      mask3 = int(TRUNCATE_MASK)\n","      for j in range(0, num_mantissa + 2):\n","        mask3 = mask3 >> 1\n","        mask3 = mask3 | 0x80000000      \n","      ###########################\n","      # select high precision code\n","      tensor_mask = tf.ones_like(convert_x)\n","      tensor_mask = tf.math.multiply(tensor_mask, mask)\n","      equal_1 = tf.equal(high_accuracy_sector, 1)\n","      tensor_mask = tf.where(equal_1, mask1, tensor_mask)\n","      equal_2 = tf.equal(high_accuracy_sector, 2)\n","      tensor_mask = tf.where(equal_2, mask2, tensor_mask)\n","      equal_3 = tf.equal(high_accuracy_sector, 3)\n","      tensor_mask = tf.where(equal_3, mask3, tensor_mask)   \n","      #print(\"mask, 1, 2, 3: \", mask, mask1, mask2, mask3)\n","      #print(\"equal_1, 2, 3: \", equal_1, equal_2, equal_3)\n","      #print(\"tensor_mask: \", tensor_mask)\n","\n","      ######### Round Code ###########\n","      ## Adding jamming code\n","      ## setting mask with 3 on bits\n","      guard_mask_save = NEAREST_PTR\n","      #for i in range(2):\n","      #  guard_mask_save = guard_mask_save >> 1 |NEAREST_PTR\n","        \n","      for j in range(num_mantissa - 1):\n","        guard_mask = guard_mask_save >> 1\n","\n","      guard_mask1 = guard_mask_save\n","      for j in range(num_mantissa):\n","        guard_mask1 = guard_mask1 >> 1\n","\n","      guard_mask2 = guard_mask_save\n","      for j in range(num_mantissa + 1):\n","        guard_mask2 = guard_mask2 >> 1\n","\n","      guard_mask3 = guard_mask_save\n","      for j in range(num_mantissa + 2):\n","        guard_mask3 = guard_mask3 >> 1\n","      ###########################\n","      # select high precision code\n","      tensor_guard_mask = tf.ones_like(convert_x)\n","      tensor_guard_mask = tf.math.multiply(tensor_mask, guard_mask)\n","      #equal_1 = tf.equal(high_accuracy_sector, 1)\n","      tensor_guard_mask = tf.where(equal_1, guard_mask1, tensor_guard_mask)\n","      #equal_2 = tf.equal(high_accuracy_sector, 2)\n","      tensor_guard_mask = tf.where(equal_2, guard_mask2, tensor_guard_mask)\n","      #equal_3 = tf.equal(high_accuracy_sector, 3)\n","      tensor_guard_mask = tf.where(equal_3, guard_mask3, tensor_guard_mask)  \n","\n","      print(\"tensor_guard_mask: \", tensor_guard_mask)\n","\n","      ##########################\n","      # Create a 1 in the lsb of each value (lsb_mask)\n","      # Use this as a mask to get the actual lsb,\n","      # zero out this mask based on the condition (actual lsb is 0 and guard bits are all 0)\n","\n","      lsb_mask = NEAREST_PTR << 1\n","      for j in range(num_mantissa - 1):\n","        lsb_mask = lsb_mask >> 1\n","\n","      lsb_mask1 = NEAREST_PTR << 1\n","      for j in range(num_mantissa):\n","        lsb_mask1 = lsb_mask1 >> 1\n","\n","      lsb_mask2 = NEAREST_PTR << 1\n","      for j in range(num_mantissa + 1):\n","        lsb_mask2 = lsb_mask2 >> 1\n","\n","      lsb_mask3 = NEAREST_PTR << 1\n","      for j in range(num_mantissa + 2):\n","        lsb_mask3 = lsb_mask3 >> 1\n","\n","      tensor_lsb_mask = tf.ones_like(convert_x)\n","      tensor_lsb_mask = tf.math.multiply(tensor_lsb_mask, lsb_mask)\n","      #equal_1 = tf.equal(high_accuracy_sector, 1)\n","      tensor_lsb_mask = tf.where(equal_1, lsb_mask1, tensor_lsb_mask)\n","      #equal_2 = tf.equal(high_accuracy_sector, 2)\n","      tensor_lsb_mask = tf.where(equal_2, lsb_mask2, tensor_lsb_mask)\n","      #equal_3 = tf.equal(high_accuracy_sector, 3)\n","      tensor_lsb_mask = tf.where(equal_3, lsb_mask3, tensor_lsb_mask)  \n","\n","      print(\"tensor_lsb_mask: \", tensor_lsb_mask)\n","      tensor_lsb_bit = bitwise_ops.bitwise_and(man_bits, tensor_lsb_mask)\n","      lsb_bit = tf.cast(tensor_lsb_bit, tf.bool)\n","      print(\"lsb_bit: \", lsb_bit)\n","      \n","      #print(\"guard_mask \", guard_mask)\n","      #print(\"tensor_guard_mask: \", tensor_guard_mask)\n","      ######### Round Code ###########\n","\n","      #print(\"guard_mask: \", guard_mask)\n","      # guard_bit = bitwise_ops.bitwise_and(man_bits, guard_mask)\n","      print(\"man_bits: \", man_bits)\n","      guard_bit = bitwise_ops.bitwise_and(man_bits, tensor_guard_mask)\n","      print(\"3 guard_bits: \", guard_bit)\n","      ######### Jamming Code\n","      # guard_bit is true if any of guard bits is a 1 -  jamming\n","      guard_bit = tf.cast(guard_bit, tf.bool)\n","      jamming_lsb = tf.logical_or(lsb_bit, guard_bit)\n","      jamming_mantissa = bitwise_ops.bitwise_or(man_bits, tensor_lsb_mask)\n","      print(\"jamming_lsb, mantissa: \", jamming_lsb, jamming_mantissa)\n","      man_bits = tf.where(jamming_lsb, jamming_mantissa, man_bits)\n","\n","      not_shift_zero = tf.math.less_equal(diff_tensor, zero_threshold)\n","      guard_bit = tf.logical_and(guard_bit, not_shift_zero)\n","\n","      #print(\"guard_bit: \", guard_bit)\n","\n","      #### Need to work on this code to enable rounding!!!\n","      # final_round_exp = tf.math.subtract(saved_exp, num_mantissa - 1)\n","      '''\n","      final_round_exp = tf.math.subtract(saved_exp, num_mantissa - 1)\n","      final_round_exp = tf.math.subtract(final_round_exp, high_accuracy_sector)\n","      final_round_exp = tf.math.add(final_round_exp, diff_tensor) \n","      final_round_exp = bitwise_ops.left_shift(final_round_exp,23)\n","      final_round_value = bitwise_ops.bitwise_or(sign_bit, final_round_exp)\n","      final_round_float = tf.bitcast(final_round_value, tf.float32)\n","      '''\n","      #print(\"final_round_float: \", final_round_float)\n","      \n","      ###############################\n","\n","      # man_bits = bitwise_ops.bitwise_and(man_bits, mask)\n","      man_bits = bitwise_ops.bitwise_and(man_bits, tensor_mask)\n","      man_bits = bitwise_ops.left_shift(man_bits, diff_tensor)\n","\n","      final_value = bitwise_ops.bitwise_or(sign_bit,exp_bits)\n","      final_value = bitwise_ops.bitwise_or(final_value, man_bits)\n","\n","      zeros_tensor = tf.zeros_like(man_bits)\n","      \n","      exp_only = tf.math.greater(diff_tensor, num_mantissa - 1)\n","      not_shift_zero = tf.math.less_equal(diff_tensor, zero_threshold)\n","      exp_only = tf.logical_and(exp_only, not_shift_zero)\n","      sign_exp_bits = bitwise_ops.bitwise_and(convert_x, SIGN_EXP_MASK)\n","      zeros_tensor = tf.where(exp_only, sign_exp_bits , zeros_tensor  )\n","\n","      final_value = tf.where(shift_to_zero, zeros_tensor, final_value)\n","\n","      #final_value = bitwise_ops.bitwise_and(final_value, mask)\n","\n","      x = tf.bitcast(final_value, tf.float32)\n","\n","      # x = tf.where(guard_bit, x + final_round_float, x )\n","\n","      if skip_processing == False:\n","        if is_tensor:\n","          t_value[t_num] = tf.constant(x)\n","        else:\n","          t_value[t_num] = x\n","      else:\n","        t_value = x\n","\n","\n","      #######################################\n","            \n","      \n","    elif (round_mode == \"afp_sector_update\"):\n","\n","      # Use the same exponent per tensor\n","      # Truncate bits that are shifted off - TODO: Look into rounding\n","\n","      convert_x = tf.bitcast(x, tf.int32)\n","\n","      sign_bit = bitwise_ops.bitwise_and(convert_x, SIGN_MASK)\n","      exp_bits = bitwise_ops.bitwise_and(convert_x, EXP_MASK)\n","      man_bits = bitwise_ops.bitwise_and(convert_x, MAN_MASK)\n","      save_man_bits = man_bits\n","\n","      exp_bits = bitwise_ops.right_shift(exp_bits,23)\n","      saved_exp = exp_bits\n","\n","      max_exp = tf.math.reduce_max(exp_bits)\n","      shifted_max_exp = bitwise_ops.left_shift(max_exp, 23)\n","\n","      max_tensor = tf.ones_like(exp_bits)\n","      max_tensor = tf.math.multiply(max_exp, max_tensor)\n","      diff_tensor = tf.math.subtract(max_tensor,exp_bits)\n","\n","      ####################\n","      # Sector Update Code\n","      # New code to profile intervals\n","      # print(\"shifted_man_bits: \", save_man_bits)\n","      interval_num = []\n","\n","      for i in range(8):\n","        interval_num.append(0)\n","\n","      shifted_man_bits = bitwise_ops.right_shift(save_man_bits, 1)\n","      shifted_man_bits = bitwise_ops.bitwise_or(shifted_man_bits, NEAREST_PTR)\n","      shifted_man_bits = bitwise_ops.right_shift(shifted_man_bits, diff_tensor)\n","      # implicit_one = tf.math.equal(exp_bits, max_exp) \n","      # implicit_mask = tf.where(implicit_one, NEAREST_PTR, 0)\n","      # shifted_man_bits = bitwise_ops.bitwise_or(shifted_man_bits, implicit_mask)\n","      truncate_mask2 = NEAREST_PTR \n","      # for i in range(num_mantissa - 1):\n","      for i in range(2):\n","        truncate_mask2 = truncate_mask2 >> 1 | NEAREST_PTR\n","\n","      #print(\"truncate_mask2: \", truncate_mask2)\n","      #print(\"shifted_man_bits: \", shifted_man_bits)\n","      interval_bits = bitwise_ops.bitwise_and(shifted_man_bits, truncate_mask2)\n","      #print(\"masked shift_man interval_bits:\")\n","      #print(interval_bits)\n","      interval_bits = bitwise_ops.right_shift(interval_bits, 20)\n","      interval_int = tf.bitcast(interval_bits, tf.int32)\n","      #print(\"interval_bits:\")\n","      #print(interval_bits)\n","      #print(\"interval_int: \", interval_bits)\n","\n","      one_hot_interval = tf.one_hot(interval_int, 8)\n","      #print(\"one_hot_interval:\")\n","      #print(one_hot_interval)\n","\n","      for i in range(8):\n","        for j in range(len(one_hot_interval)):\n","          interval_num[i] += one_hot_interval[j].numpy()\n","\n","      #####\n","      interval_bool = deepcopy(interval_num[0])\n","      #interval_bool = deepcopy(interval_num)\n","      free_encodings = 0\n","      for i in range(8):\n","        if interval_bool[i] > 0:\n","          interval_bool[i] = 1\n","        else:\n","          free_encodings += 1\n","      #print(\"Free Encodings: \", free_encodings)\n","\n","      ##############\n","      # All same sign update\n","      min_value = tf.math.reduce_min(x)\n","      max_value = tf.math.reduce_max(x)\n","      all_positive = min_value >= 0\n","      all_negative = max_value <= 0\n","      all_one_sign = all_positive or all_negative\n","\n","      ##################\n","      # range code\n","      abs_x = tf.math.abs(x)\n","      max_abs_value = tf.math.reduce_max(abs_x)\n","      min_abs_value = tf.math.reduce_min(abs_x)\n","\n","      # Code to check if min value in block is > 0001 assuming 3-bit mantissa\n","      min_over_half = max_exp - 3\n","      min_over_half = bitwise_ops.left_shift(min_over_half, 23)\n","      min_over_half = tf.bitcast(min_over_half, tf.float32)\n","      min_just_over_half = min_abs_value > min_over_half\n","      # Code to check for round down to 2^max_exp\n","      \n","      range_mask1 = int(NEAREST_PTR) >> 2\n","      # use num_mantissa - 2 to find LSB, use num_mantissa - 1 to find bit to right of LSB to check for rounding down\n","      #for j in range(0, num_mantissa-1):\n","      #  range_mask1 = range_mask1 >> 1\n","      \n","      # truncate max_exp - instead of checking for 1001 (for 3 bit mantissa)\n","      # check for 100\n","      # over_half = shifted_max_exp # bitwise_ops.left_shift(max_exp, 23)\n","\n","      over_half = bitwise_ops.bitwise_or(shifted_max_exp, range_mask1)\n","\n","      just_over_half = tf.bitcast(over_half, tf.float32)\n","\n","      high_exp = max_exp\n","      if (high_exp != 0):\n","        high_exp -= 127\n","\n","      round_down_to_half = max_abs_value < just_over_half\n","      round_down_to_one_inc = False\n","      round_down_to_two_inc = False\n","\n","      extra_num_exp = 0\n","      max_value_float = 0.0\n","      max_mantissa = 0\n","\n","      # If there are gaps in the double precision\n","      # high threshold describes the upper bound for double precision\n","      high_threshold = 0.0\n","      low_threshold = 0.0\n","      min_mantissa = 0\n","      high_precision_gap = False\n","\n","      if (all_one_sign == True):\n","        # num_mantissa += 1\n","        free_encodings += 8\n","\n","      original_num = num_mantissa\n","      #print(\"New block - remaining free encodings: \", free_encodings)\n","      # Code to check for 2x precision - equivalent to 1 extra bit\n","      if (free_encodings > 0):\n","        if all_one_sign == False:\n","          required_encodings = 8 - free_encodings\n","        else:\n","          required_encodings = 16 - free_encodings\n","\n","        while (free_encodings >= required_encodings):\n","          num_mantissa += 1\n","          free_encodings -= required_encodings\n","          required_encodings = 2*required_encodings\n","\n","      #print(\"num_mantissa: (original), (new) \", original_num, num_mantissa)\n","      #print(\"remaining free encodings: \", free_encodings)\n","      # check for remaining free encodings\n","\n","      if (free_encodings > 0):\n","        int_num = 0\n","        # Need to add code here to selectively increase accuracy for sectors\n","        # interval_bool = 2 for sections with higher accuracy\n","        while free_encodings > 0:\n","          if interval_bool[int_num] >= 1:\n","            interval_bool[int_num] += 1\n","            free_encodings -= 1\n","          int_num = (int_num + 1) % 8\n","\n","      ############################\n","      # Create tensor with flags specific to each value to indicate increase in accuracy\n","      # Flag to indicate select range for higher precision\n","      # high_accuracy_sector stores the number of extra bits of precision for the sector\n","      high_accuracy_sector = tf.zeros_like(convert_x)\n","      for i in range(8):\n","        temp_bool = tf.math.equal(interval_int, i)\n","        high_accuracy_sector = tf.where(temp_bool, interval_bool[i] -1 , high_accuracy_sector)\n","\n","      negative_sector = tf.math.less(high_accuracy_sector, 0)\n","      high_accuracy_sector = tf.where(negative_sector, 0, high_accuracy_sector)\n","      #print(\"high_accuracy_sector: \", high_accuracy_sector)\n","      ############################\n","\n","      #print(\"after while loop\")\n","      if (interval_bool[0] > 0):\n","        zero_threshold = num_mantissa - 1 + interval_bool[0] - 1\n","      else:\n","        zero_threshold = num_mantissa - 1  \n","      shift_to_zero = tf.math.greater(diff_tensor, zero_threshold) \n","\n","      # Based on the difference of the exponent, round the bits shifted - start with truncation\n","      exp_bits = bitwise_ops.left_shift(exp_bits,23)\n","      man_bits = bitwise_ops.right_shift(man_bits, diff_tensor)\n","\n","      # Need to mask out the bits lower than the num_mantissa bits\n","      mask = int(TRUNCATE_MASK)\n","      for j in range(0, num_mantissa-1):\n","        mask = mask >> 1\n","        mask = mask | 0x80000000\n","\n","      mask1 = int(TRUNCATE_MASK)\n","      for j in range(0, num_mantissa):\n","        mask1 = mask1 >> 1\n","        mask1 = mask1 | 0x80000000\n","\n","      mask2 = int(TRUNCATE_MASK)\n","      for j in range(0, num_mantissa + 1):\n","        mask2 = mask2 >> 1\n","        mask2 = mask2 | 0x80000000\n","      \n","      mask3 = int(TRUNCATE_MASK)\n","      for j in range(0, num_mantissa + 2):\n","        mask3 = mask3 >> 1\n","        mask3 = mask3 | 0x80000000      \n","      ###########################\n","      # select high precision code\n","      tensor_mask = tf.ones_like(convert_x)\n","      tensor_mask = tf.math.multiply(tensor_mask, mask)\n","      equal_1 = tf.equal(high_accuracy_sector, 1)\n","      tensor_mask = tf.where(equal_1, mask1, tensor_mask)\n","      equal_2 = tf.equal(high_accuracy_sector, 2)\n","      tensor_mask = tf.where(equal_2, mask2, tensor_mask)\n","      equal_3 = tf.equal(high_accuracy_sector, 3)\n","      tensor_mask = tf.where(equal_3, mask3, tensor_mask)   \n","      #print(\"mask, 1, 2, 3: \", mask, mask1, mask2, mask3)\n","      #print(\"equal_1, 2, 3: \", equal_1, equal_2, equal_3)\n","      #print(\"tensor_mask: \", tensor_mask)\n","\n","      ######### Round Code ###########\n","      guard_mask = NEAREST_PTR\n","      for j in range(num_mantissa - 1):\n","        guard_mask = guard_mask >> 1\n","\n","      guard_mask1 = NEAREST_PTR\n","      for j in range(num_mantissa):\n","        guard_mask1 = guard_mask1 >> 1\n","\n","      guard_mask2 = NEAREST_PTR\n","      for j in range(num_mantissa + 1):\n","        guard_mask2 = guard_mask2 >> 1\n","\n","      guard_mask3 = NEAREST_PTR\n","      for j in range(num_mantissa + 2):\n","        guard_mask3 = guard_mask3 >> 1\n","      ###########################\n","      # select high precision code\n","      tensor_guard_mask = tf.ones_like(convert_x)\n","      tensor_guard_mask = tf.math.multiply(tensor_mask, guard_mask)\n","      equal_1 = tf.equal(high_accuracy_sector, 1)\n","      tensor_guard_mask = tf.where(equal_1, guard_mask1, tensor_guard_mask)\n","      equal_2 = tf.equal(high_accuracy_sector, 2)\n","      tensor_guard_mask = tf.where(equal_2, guard_mask2, tensor_guard_mask)\n","      equal_3 = tf.equal(high_accuracy_sector, 3)\n","      tensor_guard_mask = tf.where(equal_3, guard_mask3, tensor_guard_mask)  \n","\n","      #print(\"guard_mask \", guard_mask)\n","      #print(\"tensor_guard_mask: \", tensor_guard_mask)\n","      ######### Round Code ###########\n","\n","      #print(\"guard_mask: \", guard_mask)\n","      # guard_bit = bitwise_ops.bitwise_and(man_bits, guard_mask)\n","      guard_bit = bitwise_ops.bitwise_and(man_bits, tensor_guard_mask)\n","      guard_bit = tf.cast(guard_bit, tf.bool)\n","      not_shift_zero = tf.math.less_equal(diff_tensor, zero_threshold)\n","      guard_bit = tf.logical_and(guard_bit, not_shift_zero)\n","\n","      #print(\"guard_bit: \", guard_bit)\n","\n","      #### Need to work on this code to enable rounding!!!\n","      # final_round_exp = tf.math.subtract(saved_exp, num_mantissa - 1)\n","\n","      final_round_exp = tf.math.subtract(saved_exp, num_mantissa - 1)\n","      final_round_exp = tf.math.subtract(final_round_exp, high_accuracy_sector)\n","      final_round_exp = tf.math.add(final_round_exp, diff_tensor) \n","      final_round_exp = bitwise_ops.left_shift(final_round_exp,23)\n","      final_round_value = bitwise_ops.bitwise_or(sign_bit, final_round_exp)\n","      final_round_float = tf.bitcast(final_round_value, tf.float32)\n","      \n","      #print(\"final_round_float: \", final_round_float)\n","      \n","      ###############################\n","\n","      # man_bits = bitwise_ops.bitwise_and(man_bits, mask)\n","      man_bits = bitwise_ops.bitwise_and(man_bits, tensor_mask)\n","      man_bits = bitwise_ops.left_shift(man_bits, diff_tensor)\n","\n","      final_value = bitwise_ops.bitwise_or(sign_bit,exp_bits)\n","      final_value = bitwise_ops.bitwise_or(final_value, man_bits)\n","\n","      zeros_tensor = tf.zeros_like(man_bits)\n","      \n","      exp_only = tf.math.greater(diff_tensor, num_mantissa - 1)\n","      not_shift_zero = tf.math.less_equal(diff_tensor, zero_threshold)\n","      exp_only = tf.logical_and(exp_only, not_shift_zero)\n","      sign_exp_bits = bitwise_ops.bitwise_and(convert_x, SIGN_EXP_MASK)\n","      zeros_tensor = tf.where(exp_only, sign_exp_bits , zeros_tensor  )\n","\n","      final_value = tf.where(shift_to_zero, zeros_tensor, final_value)\n","\n","      #final_value = bitwise_ops.bitwise_and(final_value, mask)\n","\n","      x = tf.bitcast(final_value, tf.float32)\n","\n","      x = tf.where(guard_bit, x + final_round_float, x )\n","\n","      if skip_processing == False:\n","        if is_tensor:\n","          t_value[t_num] = tf.constant(x)\n","        else:\n","          t_value[t_num] = x\n","      else:\n","        t_value = x\n","\n","\n","    elif (round_mode == \"afp_optimal_range_msfp_nearest\"):\n","\n","      # Use the same exponent per tensor\n","      # Truncate bits that are shifted off - TODO: Look into rounding\n","\n","      convert_x = tf.bitcast(x, tf.int32)\n","      #print(\"x: \", x)\n","\n","      sign_bit = bitwise_ops.bitwise_and(convert_x, SIGN_MASK)\n","      exp_bits = bitwise_ops.bitwise_and(convert_x, EXP_MASK)\n","      man_bits = bitwise_ops.bitwise_and(convert_x, MAN_MASK)\n","      save_man_bits = man_bits\n","\n","      exp_bits = bitwise_ops.right_shift(exp_bits,23)\n","      saved_exp = exp_bits\n","\n","      max_exp = tf.math.reduce_max(exp_bits)\n","      shifted_max_exp = bitwise_ops.left_shift(max_exp, 23)\n","\n","      max_tensor = tf.ones_like(exp_bits)\n","      max_tensor = tf.math.multiply(max_exp, max_tensor)\n","      diff_tensor = tf.math.subtract(max_tensor,exp_bits)\n","\n","      temp_num_mantissa = num_mantissa + 1\n","      inside_inc1_range = tf.zeros_like(exp_bits)\n","      inside_inc1_range = tf.cast(inside_inc1_range, tf.bool)\n","      inside_inc2_range = tf.zeros_like(exp_bits)\n","      inside_inc2_range = tf.cast(inside_inc2_range, tf.bool)\n","\n","      ##############\n","      # All same sign update\n","      min_value = tf.math.reduce_min(x)\n","      max_value = tf.math.reduce_max(x)\n","      all_positive = min_value >= 0\n","      all_negative = max_value <= 0\n","      all_one_sign = all_positive or all_negative\n","\n","      ##################\n","      # range code\n","      abs_x = tf.math.abs(x)\n","      max_abs_value = tf.math.reduce_max(abs_x)\n","      min_abs_value = tf.math.reduce_min(abs_x)\n","\n","      # Code to check if min value in block is > 0001 assuming 3-bit mantissa\n","      min_over_half = max_exp - 3\n","      min_over_half = bitwise_ops.left_shift(min_over_half, 23)\n","      min_over_half = tf.bitcast(min_over_half, tf.float32)\n","      min_just_over_half = min_abs_value > min_over_half\n","      # Code to check for round down to 2^max_exp\n","      \n","      range_mask1 = int(NEAREST_PTR) >> 2\n","      # use num_mantissa - 2 to find LSB, use num_mantissa - 1 to find bit to right of LSB to check for rounding down\n","      #for j in range(0, num_mantissa-1):\n","      #  range_mask1 = range_mask1 >> 1\n","      \n","      # truncate max_exp - instead of checking for 1001 (for 3 bit mantissa)\n","      # check for 100\n","      # over_half = shifted_max_exp # bitwise_ops.left_shift(max_exp, 23)\n","\n","      over_half = bitwise_ops.bitwise_or(shifted_max_exp, range_mask1)\n","\n","      just_over_half = tf.bitcast(over_half, tf.float32)\n","\n","      high_exp = max_exp\n","      if (high_exp != 0):\n","        high_exp -= 127\n","\n","      round_down_to_half = max_abs_value < just_over_half\n","      round_down_to_one_inc = False\n","      round_down_to_two_inc = False\n","\n","      extra_num_exp = 0\n","      max_value_float = 0.0\n","      max_mantissa = 0\n","\n","      # If there are gaps in the double precision\n","      # high threshold describes the upper bound for double precision\n","      high_threshold = 0.0\n","      low_threshold = 0.0\n","      min_mantissa = 0\n","      high_precision_gap = False\n","\n","      if (all_one_sign == True):\n","          num_mantissa += 1\n","\n","      if round_down_to_half == True:\n","        # approximation on double precision\n","        # Instead of doubling precision for all values - try capturing smaller values\n","\n","        # New code for max value float\n","        max_value = shifted_max_exp\n","        max_value_float = tf.bitcast(max_value, tf.float32)\n","\n","        max_mantissa = 0\n","        if (min_just_over_half == True): \n","          num_mantissa += 1\n","\n","        else:\n","          num_mantissa += 1\n","\n","          range_mask1 = NEAREST_PTR\n","          if (num_mantissa -1 >=3):\n","            for i in range(num_mantissa -1 -3):\n","              range_mask1 = range_mask1 >> 1 | NEAREST_PTR\n","\n","          low_thresh_exp = max_exp -1\n","          low_thresh_exp = bitwise_ops.left_shift(low_thresh_exp, 23)\n","          low_threshold = bitwise_ops.bitwise_or(low_thresh_exp, range_mask1)\n","          low_precision_threshold = tf.bitcast(low_threshold, tf.float32)\n","\n","          high_precision_gap = True\n","          #print(\"high_precision_gap == True\")\n","\n","        # keeping values with exponent only for 1/16, 1/32, 1/64\n","        #extra_num_exp = 3\n","\n","      else:\n","        # check for round down to max_exp + 1 increment\n","\n","        range_mask1 = int(NEAREST_PTR) >> 1| NEAREST_PTR\n","        range_mask1 = range_mask1 >> 1 \n","        \n","        #range_mask1 = range_mask1 | NEAREST_PTR\n","        #for j in range(0, num_mantissa-3):        \n","        #  range_mask1 = range_mask1 >> 1\n","        # use mantissa of 011 to check\n","\n","        #round_down = shifted_max_exp # bitwise_ops.left_shift(max_exp, 23)\n","        round_down = bitwise_ops.bitwise_or(shifted_max_exp, range_mask1)\n","        round_down_float = tf.bitcast(round_down, tf.float32)        \n","\n","        round_down_to_one_inc = max_abs_value < round_down_float\n","\n","        if round_down_to_one_inc == True:\n","          #extra_num_exp += 2* (2 ** (num_mantissa -3))\n","          if (min_just_over_half == True):\n","            '''\n","            if (num_mantissa < 5):\n","              extra_num_exp += 1 \n","            else:\n","              extra_num_exp += 2 ** (num_mantissa -4)\n","            '''\n","            num_mantissa += 1\n","\n","            range_mask1 = NEAREST_PTR\n","            if (num_mantissa -1 >=3):\n","              for i in range(num_mantissa -1 -3):\n","                range_mask1 = range_mask1 >> 1 | NEAREST_PTR\n","\n","            low_thresh_exp = max_exp -1\n","            low_thresh_exp = bitwise_ops.left_shift(low_thresh_exp, 23)\n","            low_threshold = bitwise_ops.bitwise_or(low_thresh_exp, range_mask1)\n","            low_precision_threshold = tf.bitcast(low_threshold, tf.float32)\n","            high_precision_gap = True   \n","\n","          else:\n","            num_mantissa += 1\n","\n","            low_thresh_exp = max_exp -1\n","            low_thresh_exp = bitwise_ops.left_shift(low_thresh_exp, 23)\n","            low_threshold = bitwise_ops.bitwise_or(low_thresh_exp, range_mask1)\n","            low_precision_threshold = tf.bitcast(low_threshold, tf.float32)\n","            high_precision_gap = True  \n","\n","          temp_man = NEAREST_PTR >> 1\n","          max_value = shifted_max_exp | temp_man\n","          max_value_float = tf.bitcast(max_value, tf.float32)\n","\n","          # max_value_float = round_down_float\n","          # max_mantissa = int(NEAREST_PTR)\n","          # max_mantissa = max_mantissa >> 1\n","       \n","        else:\n","          range_mask1 = int(NEAREST_PTR) | NEAREST_PTR >> 2\n","          #range_mask1 = range_mask1 >> 1\n","          #range_mask1 = range_mask1 | NEAREST_PTR\n","          \n","          #for j in range(0, num_mantissa-3):        \n","          #  range_mask1 = range_mask1 >> 1\n","                        # use mantissa of 101 to check\n","\n","          #round_down = shifted_max_exp #bitwise_ops.left_shift(max_exp, 23)\n","          round_down = bitwise_ops.bitwise_or(shifted_max_exp, range_mask1)\n","          round_down_float = tf.bitcast(round_down, tf.float32)        \n","\n","          round_down_to_two_inc = max_abs_value < round_down_float\n","\n","          if round_down_to_two_inc == True:\n","            #extra_num_exp += 1* (2 ** (num_mantissa -3))\n","            if (min_just_over_half == True):\n","              num_mantissa += 1\n","\n","              low_thresh_exp = max_exp -1\n","              low_thresh_exp = bitwise_ops.left_shift(low_thresh_exp, 23)\n","              low_threshold = bitwise_ops.bitwise_or(low_thresh_exp, range_mask1)\n","              low_precision_threshold = tf.bitcast(low_threshold, tf.float32)\n","              high_precision_gap = True  \n","            else:\n","              extra_num_exp += 1* (2 ** (num_mantissa -3))\n","          else:\n","            if (min_just_over_half == True):\n","              if (num_mantissa < 5):\n","                extra_num_exp += 1 \n","              else:\n","                extra_num_exp += 2 ** (num_mantissa -4)            \n","            \n","\n","            temp_man = NEAREST_PTR\n","            max_value = shifted_max_exp | temp_man\n","            max_value_float = tf.bitcast(max_value, tf.float32)\n","\n","      round_down_flag = tf.math.logical_or(round_down_to_half, round_down_to_one_inc)\n","      round_down_flag = tf.math.logical_or(round_down_flag, round_down_to_two_inc)\n","      #########################\n","      # Add code to round numbers based on reduced range before passing\n","      #########################\n","\n","      # Allow a 1 in the LSB-1 position to be rounded up\n","      # TODO: Need to check its's 11\n","      new_shift_to_lsb = tf.math.equal(diff_tensor, num_mantissa + extra_num_exp)\n","      new_shift_to_lsb_guard = bitwise_ops.bitwise_and(NEAREST_PTR, save_man_bits)\n","      new_shift_to_lsb_guard = tf.cast(new_shift_to_lsb_guard, tf.bool)\n","      new_shift_to_lsb = tf.math.logical_and(new_shift_to_lsb, new_shift_to_lsb_guard)\n","\n","      new_shift_to_lsb_value = saved_exp + 1\n","      new_shift_to_lsb_value = bitwise_ops.left_shift(new_shift_to_lsb_value, 23)\n","      new_shift_to_lsb_value = bitwise_ops.bitwise_or(new_shift_to_lsb_value, sign_bit)\n","\n","\n","      low_exp_only_range = tf.math.greater(diff_tensor, num_mantissa - 1)\n","      high_exp_only_range = tf.math.less(diff_tensor, num_mantissa + extra_num_exp)\n","      in_exp_only_range = tf.math.logical_and(low_exp_only_range, high_exp_only_range)\n","\n","      new_shift_to_zero = tf.math.greater(diff_tensor, num_mantissa-1 + extra_num_exp) \n","      shift_to_zero = tf.math.greater(diff_tensor, num_mantissa-1)\n","      #print(\"diff_tensor: \", diff_tensor)\n","      #print(\"num_mantissa: \", num_mantissa)\n","\n","      #save_exp_bits = exp_bits\n","      # Based on the difference of the exponent, round the bits shifted - start with truncation\n","      temp_exp_bits = bitwise_ops.left_shift(exp_bits,23)\n","      temp_man_bits = bitwise_ops.right_shift(man_bits, diff_tensor)\n","\n","      #saved_man_bits = man_bits\n","      # Need to mask out the bits lower than the num_mantissa bits\n","      mask = int(TRUNCATE_MASK)\n","      for j in range(0, num_mantissa-1):\n","        mask = mask >> 1\n","        mask = mask | 0x80000000\n","      \n","      temp_man_bits = bitwise_ops.bitwise_and(temp_man_bits, mask)\n","\n","      temp_man_bits = bitwise_ops.left_shift(temp_man_bits, diff_tensor)\n","\n","      final_value = bitwise_ops.bitwise_or(sign_bit,temp_exp_bits)\n","      final_value = bitwise_ops.bitwise_or(final_value, temp_man_bits)\n","\n","      final_float = tf.bitcast(final_value, tf.float32)\n","\n","      ###\n","\n","      exp_only_value = bitwise_ops.bitwise_and(convert_x, SIGN_EXP_MASK)\n","      exp_only_float = tf.bitcast(exp_only_value, tf.float32)\n","\n","      exp_only_guard = bitwise_ops.bitwise_and(NEAREST_PTR, save_man_bits)\n","      exp_only_guard = tf.cast(exp_only_guard, tf.bool)\n","\n","      exp_only_round_float = tf.math.add(exp_only_float, exp_only_float)\n","      exp_only_float = tf.where(exp_only_guard, exp_only_round_float, exp_only_float)\n","\n","      #print(\"Final Float after truncation:\" , final_float)\n","      ##################################\n","      # Increased precision range code\n","      '''\n","      mask = int(TRUNCATE_MASK)\n","      for j in range(0, temp_num_mantissa-1):\n","        mask = mask >> 1\n","        mask = mask | 0x80000000\n","\n","      temp_man_bits = bitwise_ops.bitwise_and(saved_man_bits, mask)\n","      temp_man_bits = bitwise_ops.left_shift(temp_man_bits, diff_tensor)\n","\n","      temp_value = bitwise_ops.bitwise_or(sign_bit,exp_bits)\n","      temp_value = bitwise_ops.bitwise_or(temp_value, temp_man_bits)\n","\n","      temp_float = tf.bitcast(temp_value, tf.float32)\n","      final_float = tf.where(inside_inc1_range, temp_float, final_float)\n","      final_float = tf.where(inside_inc2_range, temp_float, final_float)\n","\n","      # Need to check for round up      \n","      '''\n","      ###################################\n","      # Add nearest rounding code\n","\n","      guard_bit1 = int(NEAREST_PTR)\n","     \n","      for j in range(0, num_mantissa-1):        \n","        guard_bit1 = guard_bit1 >> 1\n","      \n","      guard_bit2 = guard_bit1 >> 1\n","\n","      save_final_float = final_float\n","\n","      #final_guard = tf.ones_like(exp_bits)\n","      final_guard = tf.where(inside_inc1_range, guard_bit2, guard_bit1)\n","      final_guard = tf.where(inside_inc2_range, guard_bit2, final_guard)     \n","      final_guard = bitwise_ops.left_shift(final_guard, diff_tensor)\n","      saved_final_guard = final_guard\n","\n","      #print(\"final_guard: \", final_guard)\n","      #print(\"save_man_bits: \", save_man_bits)\n","      final_guard = bitwise_ops.bitwise_and(final_guard, save_man_bits)\n","\n","      implicit_one_guard = tf.math.equal(diff_tensor, num_mantissa)\n","\n","      #  Comment out to allow extra exponent values to work\n","      #shift_to_zero = tf.where(implicit_one_guard, False, shift_to_zero)\n","\n","      #implicit_one_guard = bitwise_ops.bitwise_and(saved_final_guard, MAN_OVERFLOW )\n","      #implicit_one_guard = tf.cast(implicit_one_guard, tf.bool)\n","      #print(\"implicit_one_guard: \", implicit_one_guard)\n","      #print(\"diff tensor: \", diff_tensor)\n","      #print(\"num_mantissa: \", num_mantissa)\n","\n","      implicit_round_value = bitwise_ops.bitwise_and(convert_x, SIGN_EXP_MASK)\n","      implicit_round_float = tf.bitcast(implicit_round_value, tf.float32)\n","      #print(\"implicit_round_float: \", implicit_round_float)\n","\n","      #final_guard = bitwise_ops.bitwise_and(final_guard, MAN_MASK)\n","      final_guard = tf.cast(final_guard, tf.bool)\n","      #print(\"final_guard: \", final_guard)\n","\n","      final_round_exp = tf.math.subtract(saved_exp, num_mantissa - 1)\n","      final_round_exp = tf.math.add(final_round_exp, diff_tensor) \n","      final_round_exp = bitwise_ops.left_shift(final_round_exp,23)\n","      final_round_value = bitwise_ops.bitwise_or(sign_bit, final_round_exp)\n","      final_round_float = tf.bitcast(final_round_value, tf.float32)\n","      #print(\"final_round_float: \", final_round_float)\n","\n","      intermediate_float = tf.where(implicit_one_guard, final_float + implicit_round_float, final_float)\n","      intermediate_float = tf.where(final_guard, final_float + final_round_float, intermediate_float)\n","\n","      # Cap max value at the same max exponent, instead of carry out into higher max exp\n","      check_value = tf.bitcast(intermediate_float, tf.int32)\n","      new_exp_bits = bitwise_ops.bitwise_and(check_value, EXP_MASK)\n","      new_exp_bits = bitwise_ops.right_shift(new_exp_bits, 23)\n","      round_down = tf.math.greater(new_exp_bits, max_exp)\n","\n","      final_float = tf.where(round_down, final_float, intermediate_float)\n","\n","      final_value = tf.bitcast(final_float, tf.int32)\n","      #################################\n","      zeros_tensor = tf.zeros_like(man_bits)\n","      zeros_tensor = tf.where(new_shift_to_zero, zeros_tensor, exp_only_value)\n","      zeros_tensor = tf.where(new_shift_to_lsb, new_shift_to_lsb_value, zeros_tensor)\n","      zeros_tensor = tf.where(in_exp_only_range, exp_only_value, zeros_tensor)\n","\n","      #print(\"new_shift_to_zero\", new_shift_to_zero)\n","      #print(\"new_shift_to_lsb \", new_shift_to_lsb)\n","      #print(\"in_exp_only_range: \", in_exp_only_range)\n","      #print(\"shift_to_zero: \", shift_to_zero)\n","      #print(\"zeros_tensor: \", zeros_tensor)\n","      final_value = tf.where(shift_to_zero, zeros_tensor, final_value)\n","\n","      if (round_down_flag == True):\n","        pass\n","        '''\n","        # This needs to take care of sign too!\n","        temp_final_float = tf.bitcast(final_value, tf.float32)\n","        temp_abs_float = tf.math.abs(temp_final_float)\n","        select_round_down = tf.math.greater(temp_abs_float, max_value_float)\n","        temp_max_value = tf.bitcast(max_value_float, tf.int32)\n","        temp_max_value = bitwise_ops.bitwise_or(temp_max_value, sign_bit)\n","        final_value = tf.where(select_round_down, temp_max_value, final_value)        \n","        '''\n","\n","      if (high_precision_gap == True):\n","        temp_final_float = tf.bitcast(final_value, tf.float32)\n","        #print(\"num_mantissa: \", num_mantissa)\n","        #print(\"x: \", temp_final_float)\n","        temp_abs_float = tf.math.abs(temp_final_float)\n","        select_lower_precision = tf.math.greater_equal(temp_abs_float, low_precision_threshold)\n","        \n","        temp_man = bitwise_ops.bitwise_and(final_value, MAN_MASK)\n","        temp_man = bitwise_ops.right_shift(temp_man, diff_tensor)\n","\n","        mask = int(TRUNCATE_MASK)\n","\n","        for j in range(0, num_mantissa-2):\n","          mask = mask >> 1\n","          mask = mask | 0x80000000\n","\n","        temp_man = bitwise_ops.bitwise_and(temp_man, mask)\n","        temp_man = bitwise_ops.left_shift(temp_man, diff_tensor)\n","\n","        low_precision_value = bitwise_ops.bitwise_and(final_value, SIGN_EXP_MASK)\n","        low_precision_value = bitwise_ops.bitwise_or(temp_man, low_precision_value)\n","        #low_precision_value = bitwise_ops.bitwise_and(final_value, mask)\n","\n","        #print(\"select_round_down: \", select_round_down)\n","        # need to incorporate sign bit\n","        temp_final_value = tf.where(select_lower_precision, low_precision_value, final_value)\n","\n","        final_value = temp_final_value           \n","\n","      final_float = tf.bitcast(final_value, tf.float32)\n","      #print(\"final x: \", final_float)\n","      '''\n","      abs_final_float = tf.math.abs(final_float)\n","\n","      cap_to_max = tf.math.greater(abs_final_float, max_value_float)\n","      new_final_value = bitwise_ops.bitwise_and(final_value, SIGN_EXP_MASK)\n","      og_mantissa = bitwise_ops.bitwise_and(final_value, MAN_MASK)\n","      new_final_mantissa = tf.where(cap_to_max, max_mantissa, og_mantissa )\n","      new_final_value = bitwise_ops.bitwise_or(new_final_value, new_final_mantissa)\n","      '''\n","      x = tf.bitcast(final_value, tf.float32)\n","\n","      if skip_processing == False:\n","        if is_tensor:\n","          t_value[t_num] = tf.constant(x)\n","        else:\n","          t_value[t_num] = x\n","      else:\n","        t_value = x\n","\n","    elif (round_mode == \"msfp_exponent_reuse_nearest\"):\n","\n","      # Use the same exponent per tensor\n","      # Truncate bits that are shifted off - TODO: Look into rounding\n","\n","      convert_x = tf.bitcast(x, tf.int32)\n","\n","      sign_bit = bitwise_ops.bitwise_and(convert_x, SIGN_MASK)\n","      exp_bits = bitwise_ops.bitwise_and(convert_x, EXP_MASK)\n","      man_bits = bitwise_ops.bitwise_and(convert_x, MAN_MASK)\n","      save_man_bits = man_bits\n","\n","      exp_bits = bitwise_ops.right_shift(exp_bits,23)\n","      saved_exp = exp_bits\n","\n","      max_exp = tf.math.reduce_max(exp_bits)\n","      max_tensor = tf.ones_like(exp_bits)\n","      max_tensor = tf.math.multiply(max_exp, max_tensor)\n","      diff_tensor = tf.math.subtract(max_tensor,exp_bits)\n","\n","      shift_to_zero = tf.math.greater(diff_tensor, num_mantissa-1) \n","\n","      save_exp_bits = exp_bits\n","      # Based on the difference of the exponent, round the bits shifted - start with truncation\n","      exp_bits = bitwise_ops.left_shift(exp_bits,23)\n","      man_bits = bitwise_ops.right_shift(man_bits, diff_tensor)\n","\n","      # Need to mask out the bits lower than the num_mantissa bits\n","      mask = int(TRUNCATE_MASK)\n","      for j in range(0, num_mantissa-1):\n","        mask = mask >> 1\n","        mask = mask | 0x80000000\n","      \n","      man_bits = bitwise_ops.bitwise_and(man_bits, mask)\n","\n","      man_bits = bitwise_ops.left_shift(man_bits, diff_tensor)\n","\n","      final_value = bitwise_ops.bitwise_or(sign_bit,exp_bits)\n","      final_value = bitwise_ops.bitwise_or(final_value, man_bits)\n","\n","      final_float = tf.bitcast(final_value, tf.float32)\n","\n","      ###################################\n","      # Add nearest rounding code\n","\n","      new_round_exp = save_exp_bits - (num_mantissa -1) + diff_tensor\n","      new_round_exp = bitwise_ops.left_shift(new_round_exp, 23)\n","      new_round_value = bitwise_ops.bitwise_or(new_round_exp, sign_bit)\n","      new_round_float = tf.bitcast(new_round_value, tf.float32)\n","      #print(\"new_round_float: \", new_round_float)\n","\n","      second_range_less = tf.math.less(diff_tensor, num_mantissa - 1)\n","      second_range_round = second_range_less\n","      #print(\"second_range_round: \", second_range_round)\n","\n","      mask4 = tf.ones_like(exp_bits)\n","      mask4 = mask4 * int(NEAREST_PTR)\n","      mask4 = bitwise_ops.right_shift(mask4, num_mantissa - diff_tensor -1 )\n","      #print(\"mask4: \", mask4)\n","\n","      zeros_tensor = tf.zeros_like(man_bits)\n","      half_one2 = bitwise_ops.bitwise_and(mask4, save_man_bits)\n","      half_one2_true = tf.cast(half_one2, tf.bool)\n","\n","      # need to look for highest exponent - not max_exp\n","      #half_one2_true = tf.math.logical_and(not_equal_max, half_one2_true)\n","\n","      half_one2_true = tf.math.logical_and(half_one2_true,second_range_round)\n","      #print(\"half_one2: \", half_one2)\n","      #print(\"half_one2_true: \", half_one2_true)\n","\n","      #not_equal_max = tf.math.not_equal(saved_exp, max_exp)\n","\n","      intermediate_float = tf.where(half_one2_true, final_float + new_round_float, final_float)\n","      check_value = tf.bitcast(intermediate_float, tf.int32)\n","      new_exp_bits = bitwise_ops.bitwise_and(check_value, EXP_MASK)\n","      new_exp_bits = bitwise_ops.right_shift(new_exp_bits, 23)\n","      round_down = tf.math.greater(new_exp_bits, max_exp)\n","\n","      final_float = tf.where(round_down, final_float, intermediate_float)\n","      final_value = tf.bitcast(final_float, tf.int32)\n","      #################################\n","\n","      zeros_tensor = tf.zeros_like(man_bits)\n","      final_value = tf.where(shift_to_zero, zeros_tensor, final_value)\n","\n","      final_value = bitwise_ops.bitwise_and(final_value, mask)\n","\n","      x = tf.bitcast(final_value, tf.float32)\n","\n","      if skip_processing == False:\n","        if is_tensor:\n","          t_value[t_num] = tf.constant(x)\n","        else:\n","          t_value[t_num] = x\n","      else:\n","        t_value = x\n","\n","\n","    elif (round_mode == \"msfp_exponent_reuse\"):\n","\n","      # Use the same exponent per tensor\n","      # Truncate bits that are shifted off - TODO: Look into rounding\n","\n","      convert_x = tf.bitcast(x, tf.int32)\n","\n","      sign_bit = bitwise_ops.bitwise_and(convert_x, SIGN_MASK)\n","      exp_bits = bitwise_ops.bitwise_and(convert_x, EXP_MASK)\n","      man_bits = bitwise_ops.bitwise_and(convert_x, MAN_MASK)\n","\n","      exp_bits = bitwise_ops.right_shift(exp_bits,23)\n","      max_exp = tf.math.reduce_max(exp_bits)\n","      max_tensor = tf.ones_like(exp_bits)\n","      max_tensor = tf.math.multiply(max_exp, max_tensor)\n","      diff_tensor = tf.math.subtract(max_tensor,exp_bits)\n","\n","      shift_to_zero = tf.math.greater(diff_tensor, num_mantissa-1) \n","\n","      # Based on the difference of the exponent, round the bits shifted - start with truncation\n","      exp_bits = bitwise_ops.left_shift(exp_bits,23)\n","      man_bits = bitwise_ops.right_shift(man_bits, diff_tensor)\n","\n","      # Need to mask out the bits lower than the num_mantissa bits\n","      mask = int(TRUNCATE_MASK)\n","      for j in range(0, num_mantissa-1):\n","        mask = mask >> 1\n","        mask = mask | 0x80000000\n","      \n","      man_bits = bitwise_ops.bitwise_and(man_bits, mask)\n","\n","      man_bits = bitwise_ops.left_shift(man_bits, diff_tensor)\n","\n","      final_value = bitwise_ops.bitwise_or(sign_bit,exp_bits)\n","      final_value = bitwise_ops.bitwise_or(final_value, man_bits)\n","\n","      zeros_tensor = tf.zeros_like(man_bits)\n","      final_value = tf.where(shift_to_zero, zeros_tensor, final_value)\n","\n","      final_value = bitwise_ops.bitwise_and(final_value, mask)\n","\n","      x = tf.bitcast(final_value, tf.float32)\n","\n","      if skip_processing == False:\n","        if is_tensor:\n","          t_value[t_num] = tf.constant(x)\n","        else:\n","          t_value[t_num] = x\n","      else:\n","        t_value = x\n","\n","\n","    elif (round_mode == \"nearest\"):\n","      # FIXME: Need to apply tf.where changes to the custom_round_not_tensor - or merge functionality\n","\n","      # TODO: Need to fix the problem with there is a round up but it causes a carry into\n","      # bit position 23 - need to adjust both exponent and mantissa\n","      # This code rounds by doing the following:\n","      # 1) truncate to the correct # of bits\n","      # 2) check the bit to the right of the last valid bit\n","      # 3) If bit to the right (half_one) is 1, then need to round up\n","      # 4) round up by creating the correct value in floating point (with correct exp and use implicit 1 in mantissa)\n","      # 5) then doing an addition.\n","      # 6) In order to do this with the tensors, first mask off that half_one bit\n","      # 7) Then shift to the right to the 0th bit position\n","      # 8) cast this bit to float using tf.cast\n","      # 9) multiply the round value with this half_one bit/float\n","      # 10) add result to truncated value \n","\n","      mask = int(NEAREST_PTR)\n","      #print(\"This is the original mask: \",hex(mask))\n","      for j in range(1, num_mantissa):\n","        mask = mask >> 1\n","        mask = mask | NEAREST_PTR\n","\n","      #print(\"This is the new mask: \", hex(mask))\n","      convert_x = tf.bitcast(x, tf.int32)\n","      #print(\"mantissa convert_x: \", bitwise_ops.bitwise_and(convert_x, MAN_MASK))\n","      truncate_x = bitwise_ops.bitwise_and(convert_x, mask)\n","      #print(\"truncate_x: \", truncate_x)\n","\n","      mask2 = int(NEAREST_PTR)\n","      #mask_low = int(LOWEST_PTR)\n","\n","      for j in range(0, num_mantissa):\n","        mask2 = mask2 >> 1\n","        # mask will have a 1 at the bit to the right of the last mantissa bit\n","\n","      #print(\"This is mask2: \", hex(mask2))\n","      # half_one stores the bit to the right of the last mantissa bit\n","      half_one = bitwise_ops.bitwise_and(convert_x, mask2)\n","      half_one = bitwise_ops.right_shift(half_one, reduce_num -1)\n","\n","      # Create a tensor of Boolean values\n","      half_bool = tf.cast(half_one, tf.bool)\n","      #print(\"half_bool:\", half_bool)\n","\n","      mantissa_bits = bitwise_ops.bitwise_and(truncate_x, MAN_MASK)\n","\n","      rest_bits = bitwise_ops.bitwise_and(convert_x, SIGN_EXP_MASK)\n","\n","      # Need to create a float for the round up value\n","      # then add to the floating point value\n","      # floating point implicit 1, so mantissa = 0\n","      \n","      exp_bits = bitwise_ops.bitwise_and(convert_x, EXP_MASK)\n","      exp_bits = bitwise_ops.right_shift(exp_bits,23)\n","\n","      # detect denormal values by looking for 1 in exponent\n","      zeros_tensor = tf.zeros_like(exp_bits)\n","      denormal = tf.math.equal(exp_bits, zeros_tensor)\n","\n","      #print(\"exp_bits: \", exp_bits)\n","\n","      #################################################################\n","      # NOTE: This is - num_mantissa to move the binary point to the \n","      # correct location for the round value\n","      ones_tensor = tf.ones_like(exp_bits)\n","      exp_bits = exp_bits - (num_mantissa * ones_tensor)\n","      denormal_round = tf.math.less(exp_bits,ones_tensor)\n","\n","      #print(\"exp_bits: \", exp_bits)\n","      exp_bits = bitwise_ops.left_shift(exp_bits,23)\n","      sign_bit = bitwise_ops.bitwise_and(convert_x,SIGN_MASK)\n","\n","      # round_up is the round value to add\n","      round_up = bitwise_ops.bitwise_or(exp_bits, sign_bit)\n","\n","      final_x = bitwise_ops.bitwise_or(mantissa_bits, rest_bits)\n","\n","      #print(\"final_x: \", final_x)\n","      x = tf.bitcast(final_x, tf.float32)\n","      round_up = tf.where(denormal_round, zeros_tensor, round_up)\n","      y = tf.bitcast(round_up, tf.float32)\n","      \n","      final_value = tf.where(half_bool, x + y, x)\n","\n","      # filter out denormal from adding \n","      final_value = tf.where(denormal, x, final_value)\n","\n","      if is_tensor:    \n","        t_value[t_num] = tf.constant(final_value)\n","      else:\n","        t_value[t_num] = final_value\n","\n","      #print(\"This is the new tensor: \", t_value[t_num].numpy())\n","\n","    elif (round_mode == \"nearest_even\"):\n","      # page 412 of Computer Arithmetic book\n","\n","      # FIXME: Need to apply tf.where changes to the custom_round_not_tensor - or merge functionality\n","\n","      # TODO: Need to fix the problem with there is a round up but it causes a carry into\n","      # bit position 23 - need to adjust both exponent and mantissa\n","      # This code rounds by doing the following:\n","      # 1) truncate to the correct # of bits\n","      # 2) check the bit to the right of the last valid bit\n","      # 3) If bit to the right (half_one) is 1, then need to round up\n","      # 4) round up by creating the correct value in floating point (with correct exp and use implicit 1 in mantissa)\n","      # 5) then doing an addition.\n","      # 6) In order to do this with the tensors, first mask off that half_one bit\n","      # 7) Then shift to the right to the 0th bit position\n","      # 8) cast this bit to float using tf.cast\n","      # 9) multiply the round value with this half_one bit/float\n","      # 10) add result to truncated value\n","\n","      mask = int(NEAREST_PTR)\n","      #print(\"This is the original mask: \",hex(mask))\n","      for j in range(1, num_mantissa):\n","        mask = mask >> 1\n","        mask = mask | NEAREST_PTR\n","\n","      #print(\"This is the new mask: \", hex(mask))\n","      convert_x = tf.bitcast(x, tf.int32)\n","      #print(\"mantissa convert_x: \", bitwise_ops.bitwise_and(convert_x, MAN_MASK))\n","      truncate_x = bitwise_ops.bitwise_and(convert_x, mask)\n","      #print(\"truncate_x: \", truncate_x)\n","\n","      mask2 = int(NEAREST_PTR)\n","      #mask_low = int(LOWEST_PTR)\n","\n","      for j in range(0, num_mantissa):\n","        mask2 = mask2 >> 1\n","        # mask will have a 1 at the bit to the right of the last mantissa bit\n","\n","      #print(\"This is mask2: \", hex(mask2))\n","      # half_one stores the bit to the right of the last mantissa bit\n","      half_one = bitwise_ops.bitwise_and(convert_x, mask2)\n","      half_one = bitwise_ops.right_shift(half_one, reduce_num -1)\n","\n","      # Create a tensor of Boolean values\n","      half_bool = tf.cast(half_one, tf.bool)\n","      #print(\"half_bool:\", half_bool)\n","\n","      mantissa_bits = bitwise_ops.bitwise_and(truncate_x, MAN_MASK)\n","\n","      rest_bits = bitwise_ops.bitwise_and(convert_x, SIGN_EXP_MASK)\n","\n","      # Need to create a float for the round up value\n","      # then add to the floating point value\n","      # floating point implicit 1, so mantissa = 0\n","      \n","      exp_bits = bitwise_ops.bitwise_and(convert_x, EXP_MASK)\n","      exp_bits = bitwise_ops.right_shift(exp_bits,23)\n","\n","      # detect denormal values by looking for 1 in exponent\n","      zeroes_tensor = tf.zeros_like(exp_bits)\n","      denormal = tf.math.equal(exp_bits, zeroes_tensor)\n","\n","      #print(\"exp_bits: \", exp_bits)\n","\n","      #################################################################\n","      # NOTE: This is - num_mantissa to move the binary point to the \n","      # correct location for the round value\n","      exp_bits = exp_bits - num_mantissa\n","\n","      #print(\"exp_bits: \", exp_bits)\n","      exp_bits = bitwise_ops.left_shift(exp_bits,23)\n","      sign_bit = bitwise_ops.bitwise_and(convert_x,SIGN_MASK)\n","\n","      # round_up is the round value to add\n","      round_up = bitwise_ops.bitwise_or(exp_bits, sign_bit)\n","\n","      final_x = bitwise_ops.bitwise_or(mantissa_bits, rest_bits)\n","\n","      #print(\"final_x: \", final_x)\n","      x = tf.bitcast(final_x, tf.float32)\n","      y = tf.bitcast(round_up, tf.float32)\n","      \n","      if (num_mantissa < 22):\n","        mask3 = 0x1\n","        for j in range(0,23 - num_mantissa - 2):\n","          mask3 = mask3 << 1\n","          mask3 = mask3 | 0x1\n","        guard_bits = bitwise_ops.bitwise_and(convert_x,mask3)\n","        not_zero = tf.math.greater(guard_bits,zeroes_tensor)\n","        equal_zero = tf.math.equal(guard_bits,zeroes_tensor)\n","\n","        mask4 = 0x1\n","        for j in range(0,23 - num_mantissa):\n","          mask4 = mask4 << 1\n","        lsb = bitwise_ops.bitwise_and(convert_x,mask4)\n","        lsb_odd = tf.math.greater(lsb, zeroes_tensor)\n","        half_bool = tf.where(lsb_even,)\n","\n","        round_equal = tf.math.logical_and(equal_zero,lsb_odd)\n","\n","        half_bool = tf.math.logical_and(half_bool, not_zero)\n","        half_bool = tf.math.logical_or(half_bool, round_equal)\n","      else:\n","        pass\n","\n","      # mask will have a 1 at the bit to the right of the last mantissa bit\n","      #print(\"x: \", x)\n","      #print(\"y: \", y)\n","      #print(\"x+y: \", x+y)\n","      final_value = tf.where(half_bool, x + y, x)\n","\n","      # filter out denormal from adding \n","      final_value = tf.where(denormal, x, final_value)\n","\n","      if is_tensor:    \n","        t_value[t_num] = tf.constant(final_value)\n","      else:\n","        t_value[t_num] = final_value\n","\n","      #print(\"This is the new tensor: \", t_value[t_num].numpy())\n","\n","    elif (round_mode == \"ibm_stochastic\"):\n","     # This implementation is based on the IBM paper:\n","     # https://arxiv.org/pdf/1812.08011.pdf\n","\n","      convert_x = tf.bitcast(x, tf.int32)\n","\n","      # Need to take the floor\n","      mask = int(TRUNCATE_MASK)\n","      remain_mask = 0x1\n","      for j in range(0, num_mantissa):\n","        mask = mask >> 1\n","        mask = mask | 0x80000000\n","      \n","      for j in range(1,reduce_num):\n","        remain_mask = remain_mask << 1\n","        remain_mask = remain_mask | 0x1\n","\n","      #print(\"remain_mask: \", remain_mask)\n","      truncate_value = bitwise_ops.bitwise_and(convert_x, mask)\n","      remainder_value = bitwise_ops.bitwise_and(convert_x, remain_mask)\n","      #print(\"remain_value: \", remainder_value)\n","\n","      round_exponent = bitwise_ops.bitwise_and(convert_x,EXP_MASK)\n","      round_exponent = bitwise_ops.right_shift(round_exponent, 23)\n","      exp_save = round_exponent\n","      round_exponent = round_exponent - num_mantissa\n","\n","      # check for denormal round value\n","      ones_tensor = tf.ones_like(round_exponent)\n","      denormal_round = tf.math.less(round_exponent, ones_tensor)\n","      zeros_tensor = tf.zeros_like(round_exponent)\n","      round_exponent = tf.where(denormal_round, zeros_tensor, round_exponent)\n","\n","      round_exponent = bitwise_ops.left_shift(round_exponent,23)\n","      round_sign = bitwise_ops.bitwise_and(convert_x,SIGN_MASK)\n","      \n","      round_value = bitwise_ops.bitwise_or(round_exponent,round_sign)\n","\n","      float_round = tf.bitcast(round_value, tf.float32)\n","      float_x = tf.bitcast(truncate_value, tf.float32)\n","      #print(\"Truncated float_x: \", float_x)\n","\n","      added_value = float_x + float_round\n","      #print(\"Added value : \", added_value)\n","\n","      # need to move MSB of remainder  to the implicit one position of bit 23\n","      # adjust the exponent based on how many bits moved over\n","      # use cast instead of bitcast\n","\n","      ############################\n","\n","      float_remainder = tf.cast(remainder_value, tf.float32)\n","      epsilon = (2**(reduce_num + 1))\n","\n","      probability = float_remainder / epsilon\n","      #print(\"prob: \", probability)\n","\n","      random_prob = tf.random.uniform(shape = x.shape, maxval = 1, dtype=tf.float32)\n","      #print(\"random_prob: \", random_prob)\n","      random_prob = random_prob + probability\n","      #print(\"random_prob + prob: \", random_prob)\n","\n","      ones_tensor = tf.ones_like(random_prob)\n","      final_probability = tf.math.greater(random_prob, ones_tensor)\n","      #print(\"final_probability: \", final_probability)\n","      \n","      #final_probability = tf.cond((probability + random_prob) > 1.0,\n","      #                            lambda: True, lambda: False)\n","\n","      final_value = tf.where(final_probability, added_value, float_x)\n","\n","      exp_bits = bitwise_ops.bitwise_and(convert_x,EXP_MASK)\n","      # detect denormal values by looking for 1 in exponent\n","      zeroes_tensor = tf.zeros_like(exp_bits)\n","      denormal = tf.math.equal(exp_bits, zeroes_tensor)\n","\n","      final_value = tf.where(denormal, float_x, added_value)\n","\n","      x = final_value\n","\n","      if is_tensor:          \n","        t_value[t_num] = tf.constant(x)\n","      else:\n","        t_value[t_num] = x\n","\n","\n","    elif (round_mode == \"jamming\"):\n","      # Jamming performs a bit-level OR of the least significant bit(LSB)\n","      # and 3 subsequent guard bits (GBs) and places the result in the LSB\n","      # Code uses the truncation code and modifies the LSB\n","\n","      # Code to truncate the mantissa to the new number of bits\n","      mask = int(TRUNCATE_MASK)\n","      #print(\"This is the original mask: \",hex(mask))\n","      for j in range(0, num_mantissa):\n","        mask = mask >> 1\n","        mask = mask | 0x80000000\n","\n","      # Need to cast float into integer format in order to apply bitwise operations\n","      # then convert back\n","      convert_x = tf.bitcast(x, tf.int32)\n","      truncate_x = bitwise_ops.bitwise_and(convert_x, mask)\n","\n","      lsb_mask = LOWEST_PTR << reduce_num \n","      if (reduce_num > 3):\n","        g_mask = GUARD_MASK << (reduce_num -3)\n","        g_right = GUARD_RIGHT << (reduce_num -3)\n","        g_mid = GUARD_MID << (reduce_num -3)\n","        g_left = GUARD_LEFT << (reduce_num -3)\n","      elif (reduce_num == 2):\n","        g_mask = 0x00000003\n","        g_right = 0x00000000\n","        g_mid = GUARD_MID \n","        g_left = GUARD_LEFT\n","      elif (reduce_num == 1):\n","        g_mask = 0x00000001\n","        g_right = 0x00000000\n","        g_mid = 0x00000000\n","        g_left = GUARD_LEFT\n","        pass\n","      else:\n","        g_mask = 0\n","        g_right = 0x00000000\n","        g_mid = 0x00000000\n","        g_left = 0x00000000\n","\n","      #print(\"lsb_mask, g_left, g_mid, g_right: \", lsb_mask, g_left, g_mid, g_right)\n","\n","      lsb = bitwise_ops.bitwise_and(convert_x, lsb_mask)\n","      #guard_bits = bitwise_ops.bitwise_and(convert_x,g_mask)\n","      guard_l = bitwise_ops.bitwise_and(convert_x,g_left)\n","      guard_m = bitwise_ops.bitwise_and(convert_x,g_mid)\n","      guard_r = bitwise_ops.bitwise_and(convert_x,g_right)\n","\n","      #print(\"guard_r: \", guard_r)\n","      #print(\"guard_m: \", guard_m)\n","      #print(\"guard_l: \", guard_l)\n","      \n","      # Shift guard bits into same bit position as LSB\n","      # To do bitwise or on all the bits and place in the same LSB position\n","      guard_l = bitwise_ops.left_shift(guard_l,1)\n","      guard_m = bitwise_ops.left_shift(guard_m,2)\n","      guard_r = bitwise_ops.left_shift(guard_r,3)\n","\n","      #print(\"guard_r: \", guard_r)\n","      #print(\"guard_m: \", guard_m)\n","      #print(\"guard_l: \", guard_l)\n","\n","      lsb = bitwise_ops.bitwise_or(lsb,guard_r)\n","      lsb = bitwise_ops.bitwise_or(lsb,guard_m)\n","      lsb = bitwise_ops.bitwise_or(lsb,guard_l)\n","\n","      final_x = bitwise_ops.bitwise_or(truncate_x, lsb) \n","      x = tf.bitcast(final_x, tf.float32)\n","\n","      if is_tensor:          \n","        t_value[t_num] = tf.constant(x)\n","      else:\n","        t_value[t_num] = x\n","      #print(\"This is the new tensor: \", t_value[t_num].numpy())\n","      #print(\"#############################\")   \n","\n","      pass   \n","    elif (round_mode == \"hw_stochastic\"):\n","      pass\n","    elif (round_mode == \"redundant\"):\n","      pass\n","    elif (round_mode == \"custom\"):\n","      pass\n","    else:\n","      print(\"Round Mode Not Found!!!!!!!!!!!!!!!\")\n","  \n","  return t_value"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"VmgJHJbGCB8W","executionInfo":{"status":"ok","timestamp":1628701414413,"user_tz":420,"elapsed":259,"user":{"displayName":"Thomas Yeh","photoUrl":"","userId":"16777654985296421037"}}},"source":["def fine_grain_custom_round(t_value, round_mode = \"truncate\", skip_processing = True, block_round_mode = \"fine_grain_2_contiguous_exponent_reuse\",\n","                            num_exp = 8, num_mantissa = 23, radix_exp = 2, block_size = 16, \n","                            radix_mantissa = 2, is_tensor = True):\n","      \n","      #elif (round_mode == \"exponent_reuse_new_fine_grain_dimension\"):\n"," \n","      # Fine grain breakup of numbers to create blocks for exponent reuse\n","      # Do a study on varying the block_size\n","      #########################################\n","      #block_size = 128\n","\n","  # radix_exp used to send tensor level max exp\n","\n","  MAN_MASK = 0x007FFFFF \n","  EXP_MASK = 0x7F800000\n","  SIGN_MASK = 0x80000000\n","  SIGN_EXP_MASK = 0xFF800000\n","  GUARD_MASK = 0x00000007\n","  GUARD_RIGHT = 0x00000001\n","  GUARD_MID = 0x00000002 \n","  GUARD_LEFT = 0x00000004\n","\n","  # This mask is shifted to the right by reduce_num\n","  # Upper bits ORd with 1's\n","  TRUNCATE_MASK = 0xFF800000\n","\n","  # This will point to the digit after the last digit to keep\n","  NEAREST_PTR = 0x00400000\n","  LOWEST_PTR = 0x00000001\n","\n","  MAN_OVERFLOW = 0x00800000\n","\n","  # Number of bits to reduce based on new number of mantissa bits\n","  reduce_num = 23 - num_mantissa\n","\n","  exp_range = num_mantissa\n","  if (block_round_mode == \"2_contiguous_exponent_reuse_nearest_exponent_only\" or\n","      block_round_mode == \"2_contiguous_exponent_reuse_nearest\" or\n","      block_round_mode == \"2_contiguous_exponent_reuse_jamming\" or\n","      block_round_mode == \"fine_grain_2_contiguous_exponent_reuse\" or \n","      block_round_mode == \"2_contiguous_exponent_reuse_implicit\" or\n","      block_round_mode == \"ideal_2_contiguous_exponent_reuse\" or\n","      \"2\" in block_round_mode):\n","    exp_range = num_mantissa * 2\n","  elif  (block_round_mode == \"1_contiguous_exponent_reuse_nearest\" or\n","          block_round_mode == \"exponent_reuse\"):\n","    exp_range = num_mantissa\n","  elif (block_round_mode == \"3_contiguous_exponent_reuse\" or\n","        block_round_mode == \"3_contiguous_exponent_reuse_nearest\" or\n","        block_round_mode == \"3_contiguous_exponent_reuse_jamming\" or\n","        block_round_mode == \"3_contiguous_exponent_reuse_nearest_exponent_only\" or\n","        block_round_mode == \"3_contiguous_exponent_reuse_implicit\" or\n","        \"3\" in block_round_mode):\n","    exp_range = num_mantissa * 3\n","  elif (\"4\" in block_round_mode):\n","    exp_range = num_mantissa * 4        \n","\n","  # need to traverse the tensor\n","  # convert tensor to numpy array\n","  for t_num in range(len(t_value)):\n","    #inner_loop = 0\n","    if is_tensor:\n","      x = t_value[t_num].numpy()\n","    else:\n","      x = t_value[t_num]\n","\n","    convert_x = tf.bitcast(x, tf.int32)\n","    exp_bits = bitwise_ops.bitwise_and(convert_x, EXP_MASK)\n","\n","    exp_bits = bitwise_ops.right_shift(exp_bits, 23)\n","    tensor_max = tf.math.reduce_max(exp_bits)\n","    tensor_min = tf.math.reduce_min(exp_bits)\n","    tensor_diff = tensor_max - tensor_min\n","\n","    #print(\"tensor level exp range: \", tensor_diff)\n","    \n","    if (tensor_diff > exp_range):\n","      smaller_blocks = True\n","    else:\n","      smaller_blocks = False\n","       \n","    # Forcing to smaller blocks!\n","    smaller_blocks = True\n","    #mean_for_printing = tf.math.reduce_mean(exp_bits)\n","    #print(\"max: \", max_for_printing, \" mean: \", mean_for_printing, \" min: \", min_for_printing)\n","\n","    #print(\"#############################\")\n","    #print(\"This is the original tensor: \", x)\n","\n","    if (smaller_blocks == False):\n","      custom_round(x, skip_processing = True, round_mode = block_round_mode, \n","                   num_mantissa=num_mantissa, is_tensor = False)\n","    else:\n","      # Breaking tensor into smaller blocks\n","      total_elements = tf.size(x).numpy()\n","      total_dimensions = x.ndim\n","      \n","      if (total_dimensions == 1) or (total_dimensions == 2 and x.shape[1] == 1):\n","        \n","        #block = x\n","        #custom_round(block, skip_processing = skip_processing, round_mode = block_round_mode, num_mantissa=num_mantissa, is_tensor = False)\n","        #x = block\n","        \n","        start = 0\n","        stop = block_size\n","        while total_elements > block_size:\n","            total_elements = total_elements - block_size\n","            block = x[start:stop]\n","            #print(\"block: \", block)\n","            block = custom_round(block, skip_processing = skip_processing, round_mode = block_round_mode, num_mantissa=num_mantissa, is_tensor = False, radix_exp = tensor_max)\n","            #print(\"after block: \", block)\n","            x[start:stop] = block\n","            start += block_size\n","            stop += block_size\n","\n","        if (total_elements != 0): # round remaining values\n","            stop = start + total_elements\n","            block = x[start:stop]\n","            block = custom_round(block,  skip_processing = skip_processing, round_mode = block_round_mode, num_mantissa=num_mantissa, is_tensor = False, radix_exp = tensor_max)\n","            x[start:stop] = block\n","        \n","      elif (total_dimensions == 2) or (total_dimensions == 3 and x.shape[2] == 1):\n","\n","        for i in range(x.shape[0]):\n","          #print(\"dim 2 inner loop: \", inner_loop)\n","          #inner_loop += 1\n","\n","          ######\n","          total_elements = x.shape[1]\n","          start = 0\n","          stop = block_size\n","\n","          \n","\n","          while total_elements > block_size:\n","              total_elements = total_elements - block_size\n","              block = x[i, start:stop]\n","              #print(\"block: \", block)\n","              block = custom_round(block, skip_processing = skip_processing, round_mode = block_round_mode, num_mantissa=num_mantissa, is_tensor = False, radix_exp = tensor_max)\n","              #print(\"after block: \", block)\n","              x[i, start:stop] = block\n","              start += block_size\n","              stop += block_size\n","\n","          if (total_elements != 0): # round remaining values\n","              stop = start + total_elements\n","              block = x[i, start:stop]\n","              block = custom_round(block,  skip_processing = skip_processing, round_mode = block_round_mode, num_mantissa=num_mantissa, is_tensor = False, radix_exp = tensor_max)\n","              x[i, start:stop] = block\n","          ######\n","          '''\n","          row_x = x[i, :]\n","          #print(\"row_x: \", row_x)\n","          row_x = custom_round(row_x,  skip_processing = skip_processing, round_mode = block_round_mode, num_mantissa=num_mantissa, is_tensor = False)\n","          #print(\"after row_x: \", row_x)\n","          x[i, :] = row_x\n","          '''\n","\n","      elif (total_dimensions == 3) or (total_dimensions == 4 and x.shape[3] == 1):\n","        for i in range(x.shape[0]):\n","          for j in range(x.shape[1]):\n","            #print(\"dim 3 inner loop: \", inner_loop)\n","            #inner_loop += 1\n","\n","            ######\n","            total_elements = x.shape[2]\n","            start = 0\n","            stop = block_size\n","            while total_elements > block_size:\n","                total_elements = total_elements - block_size\n","                block = x[i, j, start:stop]\n","                #print(\"block: \", block)\n","                block = custom_round(block, skip_processing = skip_processing, round_mode = block_round_mode, num_mantissa=num_mantissa, is_tensor = False, radix_exp = tensor_max)\n","                #print(\"after block: \", block)\n","                x[i, j, start:stop] = block\n","                start += block_size\n","                stop += block_size\n","                #if (total_dimensions == 4):\n","                #  print(\"block: \", block)\n","\n","            if (total_elements != 0): # round remaining values\n","                stop = start + total_elements\n","                block = x[i, j, start:stop]\n","                block = custom_round(block,  skip_processing = skip_processing, round_mode = block_round_mode, num_mantissa=num_mantissa, is_tensor = False, radix_exp = tensor_max)\n","                x[i, j, start:stop] = block\n","            ######\n","            '''\n","            row_x = x[i,j, :]\n","            #print(\"row_x: \", row_x)\n","            row_x = custom_round(row_x,  skip_processing = skip_processing, round_mode = block_round_mode, num_mantissa=num_mantissa, is_tensor = False)\n","            #print(\"after row_x: \", row_x)\n","            x[i, j, :] = row_x\n","            '''\n","\n","      elif (total_dimensions == 4):\n","        for i in range(x.shape[0]):\n","          for j in range(x.shape[1]):\n","            for k in range(x.shape[2]):\n","              #print(\"dim 4 inner loop: \", inner_loop)\n","              #inner_loop += 1\n","\n","              ######\n","              total_elements = x.shape[3]\n","              start = 0\n","              stop = block_size\n","              while total_elements > block_size:\n","                  total_elements = total_elements - block_size\n","                  block = x[i, j, k, start:stop]\n","                  #print(\"block: \", block)\n","                  block = custom_round(block, skip_processing = skip_processing, round_mode = block_round_mode, num_mantissa=num_mantissa, is_tensor = False, radix_exp = tensor_max)\n","                  #print(\"after block: \", block)\n","                  x[i, j, k, start:stop] = block\n","                  start += block_size\n","                  stop += block_size\n","\n","              if (total_elements != 0): # round remaining values\n","                  stop = start + total_elements\n","                  block = x[i, j, k, start:stop]\n","                  block = custom_round(block,  skip_processing = skip_processing, round_mode = block_round_mode, num_mantissa=num_mantissa, is_tensor = False, radix_exp = tensor_max)\n","                  x[i, j, k, start:stop] = block\n","              ######\n","              '''\n","              row_x = x[i,j,k, :]\n","              #print(\"row_x: \", row_x)\n","              #print(\"x row: \", x[i,j,k, :])\n","              row_x = custom_round(row_x,  skip_processing = skip_processing, round_mode = block_round_mode, num_mantissa=num_mantissa, is_tensor = False)\n","              #print(\"after row_x: \", row_x)\n","              x[i, j, k, :] = row_x\n","              #print(\"after x row: \", x[i,j,k, :])\n","              '''\n","      else:\n","        print(\"Warning: tensor with dimension > 4!!!\")\n","\n","    if is_tensor:          \n","      t_value[t_num] = tf.constant(x)\n","    else:\n","      t_value[t_num] = x\n","\n","  #print(\"#############################\")\n","  #print(\"This is the new tensor: \", x)          \n","  return t_value  "],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"DJN5Ko4KeSX8","executionInfo":{"status":"ok","timestamp":1628701415366,"user_tz":420,"elapsed":955,"user":{"displayName":"Thomas Yeh","photoUrl":"","userId":"16777654985296421037"}}},"source":["def new_fine_grain_custom_round(t_value, round_mode = \"truncate\", skip_processing = True, block_round_mode = \"fine_grain_2_contiguous_exponent_reuse\",\n","                            num_exp = 8, num_mantissa = 23, radix_exp = 2, block_size = 16, \n","                            radix_mantissa = 2, is_tensor = True):\n","      \n","      #elif (round_mode == \"exponent_reuse_new_fine_grain_dimension\"):\n"," \n","      # Fine grain breakup of numbers to create blocks for exponent reuse\n","      # Do a study on varying the block_size\n","      #########################################\n","      #block_size = 128\n","\n","  MAN_MASK = 0x007FFFFF \n","  EXP_MASK = 0x7F800000\n","  SIGN_MASK = 0x80000000\n","  SIGN_EXP_MASK = 0xFF800000\n","  GUARD_MASK = 0x00000007\n","  GUARD_RIGHT = 0x00000001\n","  GUARD_MID = 0x00000002 \n","  GUARD_LEFT = 0x00000004\n","\n","  # This mask is shifted to the right by reduce_num\n","  # Upper bits ORd with 1's\n","  TRUNCATE_MASK = 0xFF800000\n","\n","  # This will point to the digit after the last digit to keep\n","  NEAREST_PTR = 0x00400000\n","  LOWEST_PTR = 0x00000001\n","\n","  MAN_OVERFLOW = 0x00800000\n","\n","  # Number of bits to reduce based on new number of mantissa bits\n","  reduce_num = 23 - num_mantissa\n","\n","  exp_range = num_mantissa\n","  if (block_round_mode == \"2_contiguous_exponent_reuse_nearest_exponent_only\" or\n","      block_round_mode == \"2_contiguous_exponent_reuse_nearest\" or\n","      block_round_mode == \"2_contiguous_exponent_reuse_jamming\" or\n","      block_round_mode == \"fine_grain_2_contiguous_exponent_reuse\" or \n","      block_round_mode == \"2_contiguous_exponent_reuse_implicit\" or\n","      block_round_mode == \"ideal_2_contiguous_exponent_reuse\" or\n","      \"2\" in block_round_mode):\n","    exp_range = num_mantissa * 2\n","  elif  (block_round_mode == \"1_contiguous_exponent_reuse_nearest\" or\n","          block_round_mode == \"exponent_reuse\"):\n","    exp_range = num_mantissa\n","  elif (block_round_mode == \"3_contiguous_exponent_reuse\" or\n","        block_round_mode == \"3_contiguous_exponent_reuse_nearest\" or\n","        block_round_mode == \"3_contiguous_exponent_reuse_jamming\" or\n","        block_round_mode == \"3_contiguous_exponent_reuse_nearest_exponent_only\" or\n","        block_round_mode == \"3_contiguous_exponent_reuse_implicit\" or\n","        \"3\" in block_round_mode):\n","    exp_range = num_mantissa * 3\n","  elif (\"4\" in block_round_mode):\n","    exp_range = num_mantissa * 4        \n","\n","  # need to traverse the tensor\n","  # convert tensor to numpy array\n","  for t_num in range(len(t_value)):\n","    #inner_loop = 0\n","    if is_tensor:\n","      x = t_value[t_num].numpy()\n","    else:\n","      x = t_value[t_num]\n","\n","    convert_x = tf.bitcast(x, tf.int32)\n","    exp_bits = bitwise_ops.bitwise_and(convert_x, EXP_MASK)\n","\n","    exp_bits = bitwise_ops.right_shift(exp_bits, 23)\n","    tensor_max = tf.math.reduce_max(exp_bits)\n","    tensor_min = tf.math.reduce_min(exp_bits)\n","    tensor_diff = tensor_max - tensor_min\n","    \n","    if (tensor_diff > exp_range):\n","      smaller_blocks = True\n","    else:\n","      smaller_blocks = False\n","       \n","    #mean_for_printing = tf.math.reduce_mean(exp_bits)\n","    #print(\"max: \", max_for_printing, \" mean: \", mean_for_printing, \" min: \", min_for_printing)\n","\n","    #print(\"#############################\")\n","    #print(\"This is the original tensor: \", x)\n","\n","    if (smaller_blocks == False):\n","      custom_round(x, skip_processing = True, round_mode = block_round_mode, \n","                   num_mantissa=num_mantissa, is_tensor = False)\n","    else:\n","      # Breaking tensor into smaller blocks\n","      total_elements = tf.size(x).numpy()\n","      total_dimensions = x.ndim\n","      \n","      if (total_dimensions == 1):\n","        \n","        #block = x\n","        #custom_round(block, skip_processing = skip_processing, round_mode = block_round_mode, num_mantissa=num_mantissa, is_tensor = False)\n","        #x = block\n","        \n","        start = 0\n","        stop = block_size\n","        while total_elements > block_size:\n","            total_elements = total_elements - block_size\n","            block = x[start:stop]\n","            #print(\"block: \", block)\n","            block = custom_round(block, skip_processing = skip_processing, round_mode = block_round_mode, num_mantissa=num_mantissa, is_tensor = False)\n","            #print(\"after block: \", block)\n","            x[start:stop] = block\n","            start += block_size\n","            stop += block_size\n","\n","        if (total_elements != 0): # round remaining values\n","            stop = start + total_elements\n","            block = x[start:stop]\n","            block = custom_round(block,  skip_processing = skip_processing, round_mode = block_round_mode, num_mantissa=num_mantissa, is_tensor = False)\n","            x[start:stop] = block\n","        \n","      elif (total_dimensions == 2):\n","\n","        # of last axis elements to take for each block if block_size > total elements\n","        num_axis_elements = block_size // x.shape[1]\n","\n","        if num_axis_elements <= 1:\n","   \n","          for i in range(x.shape[0]):\n","            #print(\"dim 2 inner loop: \", inner_loop)\n","            #inner_loop += 1\n","\n","            total_elements = x.shape[1]\n","            start = 0\n","            stop = block_size\n","\n","            while total_elements > block_size:\n","                total_elements = total_elements - block_size\n","                block = x[i, start:stop]\n","                #print(\"block: \", block)\n","                block = custom_round(block, skip_processing = skip_processing, round_mode = block_round_mode, num_mantissa=num_mantissa, is_tensor = False)\n","                #print(\"after block: \", block)\n","                x[i, start:stop] = block\n","                start += block_size\n","                stop += block_size\n","\n","            if (total_elements != 0): # round remaining values\n","                stop = start + total_elements\n","                block = x[i, start:stop]\n","                block = custom_round(block,  skip_processing = skip_processing, round_mode = block_round_mode, num_mantissa=num_mantissa, is_tensor = False)\n","                x[i, start:stop] = block\n","\n","        else:\n","          # Multiple last axis elements for each block\n","\n","            total_elements = x.shape[0]\n","            start = 0\n","            stop = num_axis_elements\n","            block_size = num_axis_elements\n","\n","            while total_elements > block_size:\n","                total_elements = total_elements - block_size\n","                block = x[start:stop, :]\n","                #print(\"block: \", block)\n","                block = custom_round(block, skip_processing = skip_processing, round_mode = block_round_mode, num_mantissa=num_mantissa, is_tensor = False)\n","                #print(\"after block: \", block)\n","                x[start:stop, :] = block\n","                start += block_size\n","                stop += block_size\n","\n","            if (total_elements != 0): # round remaining values\n","                stop = start + total_elements\n","                block = x[start:stop, :]\n","                block = custom_round(block,  skip_processing = skip_processing, round_mode = block_round_mode, num_mantissa=num_mantissa, is_tensor = False)\n","                x[start:stop,:] = block\n","\n","\n","      elif (total_dimensions == 3):\n","\n","        num_axis_elements = block_size // x.shape[2]\n","\n","        if num_axis_elements <= 1:\n","\n","          for i in range(x.shape[0]):\n","            for j in range(x.shape[1]):\n","              #print(\"dim 3 inner loop: \", inner_loop)\n","              #inner_loop += 1\n","\n","              ######\n","              total_elements = x.shape[2]\n","              start = 0\n","              stop = block_size\n","              while total_elements > block_size:\n","                  total_elements = total_elements - block_size\n","                  block = x[i, j, start:stop]\n","                  #print(\"block: \", block)\n","                  block = custom_round(block, skip_processing = skip_processing, round_mode = block_round_mode, num_mantissa=num_mantissa, is_tensor = False)\n","                  #print(\"after block: \", block)\n","                  x[i, j, start:stop] = block\n","                  start += block_size\n","                  stop += block_size\n","\n","              if (total_elements != 0): # round remaining values\n","                  stop = start + total_elements\n","                  block = x[i, j, start:stop]\n","                  block = custom_round(block,  skip_processing = skip_processing, round_mode = block_round_mode, num_mantissa=num_mantissa, is_tensor = False)\n","                  x[i, j, start:stop] = block\n"," \n","        else:\n","            num_axis_elements2 = num_axis_elements // x.shape[1]\n","\n","            for i in range(x.shape[0]):\n","              total_elements = x.shape[1]\n","              start = 0\n","              stop = num_axis_elements2\n","              block_size = num_axis_elements2\n","\n","              while total_elements > block_size:\n","                  total_elements = total_elements - block_size\n","                  block = x[i, start:stop, :]\n","                  #print(\"block: \", block)\n","                  block = custom_round(block, skip_processing = skip_processing, round_mode = block_round_mode, num_mantissa=num_mantissa, is_tensor = False)\n","                  #print(\"after block: \", block)\n","                  x[i, start:stop, :] = block\n","                  start += block_size\n","                  stop += block_size\n","\n","              if (total_elements != 0): # round remaining values\n","                  stop = start + total_elements\n","                  block = x[i, start:stop, :]\n","                  block = custom_round(block,  skip_processing = skip_processing, round_mode = block_round_mode, num_mantissa=num_mantissa, is_tensor = False)\n","                  x[i, start:stop, :] = block\n","\n","\n","\n","      elif (total_dimensions == 4):\n","\n","        num_axis_elements = block_size // x.shape[3]\n","\n","        if num_axis_elements <= 1:\n","\n","          for i in range(x.shape[0]):\n","            for j in range(x.shape[1]):\n","              for k in range(x.shape[2]):\n","                #print(\"dim 4 inner loop: \", inner_loop)\n","                #inner_loop += 1\n","\n","                ######\n","                total_elements = x.shape[3]\n","                start = 0\n","                stop = block_size\n","                while total_elements > block_size:\n","                    total_elements = total_elements - block_size\n","                    block = x[i, j, k, start:stop]\n","                    #print(\"block: \", block)\n","                    block = custom_round(block, skip_processing = skip_processing, round_mode = block_round_mode, num_mantissa=num_mantissa, is_tensor = False)\n","                    #print(\"after block: \", block)\n","                    x[i, j, k, start:stop] = block\n","                    start += block_size\n","                    stop += block_size\n","\n","                if (total_elements != 0): # round remaining values\n","                    stop = start + total_elements\n","                    block = x[i, j, k, start:stop]\n","                    block = custom_round(block,  skip_processing = skip_processing, round_mode = block_round_mode, num_mantissa=num_mantissa, is_tensor = False)\n","                    x[i, j, k, start:stop] = block\n","\n","        else:\n","\n","          num_axis_elements2 = num_axis_elements // x.shape[2]\n","\n","          for i in range(x.shape[0]):\n","            for j in range(x.shape[1]):\n","\n","              total_elements = x.shape[2]\n","              start = 0\n","              stop = num_axis_elements2\n","              block_size = num_axis_elements2              \n","\n","              while total_elements > block_size:\n","                  total_elements = total_elements - block_size\n","                  block = x[i, j, start:stop, :]\n","                  #print(\"block: \", block)\n","                  block = custom_round(block, skip_processing = skip_processing, round_mode = block_round_mode, num_mantissa=num_mantissa, is_tensor = False)\n","                  #print(\"after block: \", block)\n","                  x[i, j, start:stop, :] = block\n","                  start += block_size\n","                  stop += block_size\n","\n","              if (total_elements != 0): # round remaining values\n","                  stop = start + total_elements\n","                  block = x[i, j, start:stop, :]\n","                  block = custom_round(block,  skip_processing = skip_processing, round_mode = block_round_mode, num_mantissa=num_mantissa, is_tensor = False)\n","                  x[i, j, start:stop, :] = block          \n","\n","\n","    if is_tensor:          \n","      t_value[t_num] = tf.constant(x)\n","    else:\n","      t_value[t_num] = x\n","\n","  #print(\"#############################\")\n","  #print(\"This is the new tensor: \", x)          \n","  return t_value  "],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kvwtq9PAYHg7","executionInfo":{"status":"ok","timestamp":1628701415633,"user_tz":420,"elapsed":269,"user":{"displayName":"Thomas Yeh","photoUrl":"","userId":"16777654985296421037"}}},"source":["def layer_outputs_custom_round(t_value, round_mode = \"truncate\", skip_processing = True, block_round_mode = \"fine_grain_2_contiguous_exponent_reuse\",\n","                              num_exp = 8, num_mantissa = 23, radix_exp = 2, block_size = 16, \n","                              radix_mantissa = 2, is_tensor = True):\n","      \n","      #elif (round_mode == \"exponent_reuse_new_fine_grain_dimension\"):\n"," \n","      # Fine grain breakup of numbers to create blocks for exponent reuse\n","      # Do a study on varying the block_size\n","      #########################################\n","      #block_size = 128\n","\n","  MAN_MASK = 0x007FFFFF \n","  EXP_MASK = 0x7F800000\n","  SIGN_MASK = 0x80000000\n","  SIGN_EXP_MASK = 0xFF800000\n","  GUARD_MASK = 0x00000007\n","  GUARD_RIGHT = 0x00000001\n","  GUARD_MID = 0x00000002 \n","  GUARD_LEFT = 0x00000004\n","\n","  # This mask is shifted to the right by reduce_num\n","  # Upper bits ORd with 1's\n","  TRUNCATE_MASK = 0xFF800000\n","\n","  # This will point to the digit after the last digit to keep\n","  NEAREST_PTR = 0x00400000\n","  LOWEST_PTR = 0x00000001\n","\n","  MAN_OVERFLOW = 0x00800000\n","\n","  # Number of bits to reduce based on new number of mantissa bits\n","  reduce_num = 23 - num_mantissa\n","\n","  exp_range = num_mantissa\n","  if (block_round_mode == \"2_contiguous_exponent_reuse_nearest_exponent_only\" or\n","      block_round_mode == \"2_contiguous_exponent_reuse_nearest\" or\n","      block_round_mode == \"2_contiguous_exponent_reuse_jamming\" or\n","      block_round_mode == \"fine_grain_2_contiguous_exponent_reuse\" or \n","      block_round_mode == \"2_contiguous_exponent_reuse_implicit\" or\n","      block_round_mode == \"ideal_2_contiguous_exponent_reuse\" or\n","      \"2\" in block_round_mode):\n","    exp_range = num_mantissa * 2\n","  elif  (block_round_mode == \"1_contiguous_exponent_reuse_nearest\" or\n","          block_round_mode == \"exponent_reuse\"):\n","    exp_range = num_mantissa\n","  elif (block_round_mode == \"3_contiguous_exponent_reuse\" or\n","        block_round_mode == \"3_contiguous_exponent_reuse_nearest\" or\n","        block_round_mode == \"3_contiguous_exponent_reuse_jamming\" or\n","        block_round_mode == \"3_contiguous_exponent_reuse_nearest_exponent_only\" or\n","        block_round_mode == \"3_contiguous_exponent_reuse_implicit\" or\n","        \"3\" in block_round_mode):\n","    exp_range = num_mantissa * 3\n","  elif (\"4\" in block_round_mode):\n","    exp_range = num_mantissa * 4        \n","\n","  # need to traverse the tensor\n","  # convert tensor to numpy array\n","  '''\n","  for t_num in range(len(t_value)):\n","    #inner_loop = 0\n","    if is_tensor:\n","      x = t_value[t_num].numpy()\n","    else:\n","      x = t_value[t_num]\n","  '''\n","\n","  x = t_value.numpy()\n","\n","  convert_x = tf.bitcast(x, tf.int32)\n","  exp_bits = bitwise_ops.bitwise_and(convert_x, EXP_MASK)\n","\n","  exp_bits = bitwise_ops.right_shift(exp_bits, 23)\n","  tensor_max = tf.math.reduce_max(exp_bits)\n","  tensor_min = tf.math.reduce_min(exp_bits)\n","  tensor_diff = tensor_max - tensor_min\n","  \n","  if (tensor_diff > exp_range):\n","    smaller_blocks = True\n","  else:\n","    smaller_blocks = False\n","      \n","  #mean_for_printing = tf.math.reduce_mean(exp_bits)\n","  #print(\"max: \", max_for_printing, \" mean: \", mean_for_printing, \" min: \", min_for_printing)\n","\n","  #print(\"#############################\")\n","  #print(\"This is the original tensor: \", x)\n","\n","  smaller_blocks = True\n","  if (smaller_blocks == False):\n","    custom_round(x, skip_processing = True, round_mode = block_round_mode, \n","                  num_mantissa=num_mantissa, is_tensor = False)\n","  else:\n","    # Breaking tensor into smaller blocks\n","    total_elements = tf.size(x).numpy()\n","    total_dimensions = x.ndim\n","    \n","    if (total_dimensions == 1):\n","      \n","      #block = x\n","      #custom_round(block, skip_processing = skip_processing, round_mode = block_round_mode, num_mantissa=num_mantissa, is_tensor = False)\n","      #x = block\n","      \n","      start = 0\n","      stop = block_size\n","      while total_elements > block_size:\n","          total_elements = total_elements - block_size\n","          block = x[start:stop]\n","          #print(\"block: \", block)\n","          block = custom_round(block, skip_processing = skip_processing, round_mode = block_round_mode, num_mantissa=num_mantissa, is_tensor = False)\n","          #print(\"after block: \", block)\n","          x[start:stop] = block\n","          start += block_size\n","          stop += block_size\n","\n","      if (total_elements != 0): # round remaining values\n","          stop = start + total_elements\n","          block = x[start:stop]\n","          block = custom_round(block,  skip_processing = skip_processing, round_mode = block_round_mode, num_mantissa=num_mantissa, is_tensor = False)\n","          x[start:stop] = block\n","      \n","    elif (total_dimensions == 2):\n","\n","      for i in range(x.shape[0]):\n","        #print(\"dim 2 inner loop: \", inner_loop)\n","        #inner_loop += 1\n","\n","        ######\n","        total_elements = x.shape[1]\n","        start = 0\n","        stop = block_size\n","        while total_elements > block_size:\n","            total_elements = total_elements - block_size\n","            block = x[i, start:stop]\n","            #print(\"block: \", block)\n","            block = custom_round(block, skip_processing = skip_processing, round_mode = block_round_mode, num_mantissa=num_mantissa, is_tensor = False)\n","            #print(\"after block: \", block)\n","            x[i, start:stop] = block\n","            start += block_size\n","            stop += block_size\n","\n","        if (total_elements != 0): # round remaining values\n","            stop = start + total_elements\n","            block = x[i, start:stop]\n","            block = custom_round(block,  skip_processing = skip_processing, round_mode = block_round_mode, num_mantissa=num_mantissa, is_tensor = False)\n","            x[i, start:stop] = block\n","        ######\n","        '''\n","        row_x = x[i, :]\n","        #print(\"row_x: \", row_x)\n","        row_x = custom_round(row_x,  skip_processing = skip_processing, round_mode = block_round_mode, num_mantissa=num_mantissa, is_tensor = False)\n","        #print(\"after row_x: \", row_x)\n","        x[i, :] = row_x\n","        '''\n","\n","    elif (total_dimensions == 3):\n","      for i in range(x.shape[0]):\n","        for j in range(x.shape[1]):\n","          #print(\"dim 3 inner loop: \", inner_loop)\n","          #inner_loop += 1\n","\n","          ######\n","          total_elements = x.shape[2]\n","          start = 0\n","          stop = block_size\n","          while total_elements > block_size:\n","              total_elements = total_elements - block_size\n","              block = x[i, j, start:stop]\n","              #print(\"block: \", block)\n","              block = custom_round(block, skip_processing = skip_processing, round_mode = block_round_mode, num_mantissa=num_mantissa, is_tensor = False)\n","              #print(\"after block: \", block)\n","              x[i, j, start:stop] = block\n","              start += block_size\n","              stop += block_size\n","\n","          if (total_elements != 0): # round remaining values\n","              stop = start + total_elements\n","              block = x[i, j, start:stop]\n","              block = custom_round(block,  skip_processing = skip_processing, round_mode = block_round_mode, num_mantissa=num_mantissa, is_tensor = False)\n","              x[i, j, start:stop] = block\n","          ######\n","          '''\n","          row_x = x[i,j, :]\n","          #print(\"row_x: \", row_x)\n","          row_x = custom_round(row_x,  skip_processing = skip_processing, round_mode = block_round_mode, num_mantissa=num_mantissa, is_tensor = False)\n","          #print(\"after row_x: \", row_x)\n","          x[i, j, :] = row_x\n","          '''\n","\n","    elif (total_dimensions == 4):\n","      for i in range(x.shape[0]):\n","        for j in range(x.shape[1]):\n","          for k in range(x.shape[2]):\n","            #print(\"dim 4 inner loop: \", inner_loop)\n","            #inner_loop += 1\n","\n","            ######\n","            total_elements = x.shape[3]\n","            start = 0\n","            stop = block_size\n","            while total_elements > block_size:\n","                total_elements = total_elements - block_size\n","                block = x[i, j, k, start:stop]\n","                #print(\"block: \", block)\n","                block = custom_round(block, skip_processing = skip_processing, round_mode = block_round_mode, num_mantissa=num_mantissa, is_tensor = False)\n","                #print(\"after block: \", block)\n","                x[i, j, k, start:stop] = block\n","                start += block_size\n","                stop += block_size\n","\n","            if (total_elements != 0): # round remaining values\n","                stop = start + total_elements\n","                block = x[i, j, k, start:stop]\n","                block = custom_round(block,  skip_processing = skip_processing, round_mode = block_round_mode, num_mantissa=num_mantissa, is_tensor = False)\n","                x[i, j, k, start:stop] = block\n","            ######\n","            '''\n","            row_x = x[i,j,k, :]\n","            #print(\"row_x: \", row_x)\n","            #print(\"x row: \", x[i,j,k, :])\n","            row_x = custom_round(row_x,  skip_processing = skip_processing, round_mode = block_round_mode, num_mantissa=num_mantissa, is_tensor = False)\n","            #print(\"after row_x: \", row_x)\n","            x[i, j, k, :] = row_x\n","            #print(\"after x row: \", x[i,j,k, :])\n","            '''\n","  '''  \n","  if is_tensor:          \n","    t_value[t_num] = tf.constant(x)\n","  else:\n","    t_value[t_num] = x\n","  '''\n","  \n","  t_value = tf.constant(x)\n","\n","#print(\"#############################\")\n","#print(\"This is the new tensor: \", x)          \n","  return t_value  "],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"R_RZ7gUx_YQ3","executionInfo":{"status":"ok","timestamp":1628701415634,"user_tz":420,"elapsed":15,"user":{"displayName":"Thomas Yeh","photoUrl":"","userId":"16777654985296421037"}}},"source":["# Verification code for custom rounding\n","testing_rounding = False\n","\n","if (testing_rounding): \n","  \n","  test_tensor = tf.constant([ \n","    [[60, 1.5, 49.75, 0.25, 64],\n","    [5.25, 6.25, 7.25, 8.25, 8.45]],\n","    [[0.32, 0.22, 0.12, 0.52, 0.92],\n","    [0.52, 0.62, 0.72, 0.82, 0.2]],\n","    [[20.2, 21.2, 22.2, 23.2, 24.2],\n","    [25.2, 26.2, 27.2, 28.2, 29.2]],])\n","\n","  test_tensor2 = tf.constant([\n","    [[40, 1.5, 49.75, 0.25, 38],\n","    [5.5, 6.5, 7.5, 8.5, 8.9]],\n","    [[10.2, 11.2, 12.2, 13.2, 14.2],\n","    [15.2, 16.2, 17.2, 18.2, 19.2]],\n","    [[20.2, 21.2, 22.2, 23.2, 24.2],\n","    [25.2, 26.2, 27.2, 28.2, 29.2]],])\n","  \n","  test_tensor3 = tf.constant([\n","    [[40, 1.5, 49.75, 0.25, 38],\n","    [5.5, 6.5, 7.5, 8.5, 8.9]],\n","    [[1.27, -2.25, 4, 6.6, 8],\n","    [15.2, 16.2, 17.2, 18.2, 19.2]],\n","    [[20.2, 21.2, 22.2, 23.2, 24.2],\n","    [25.2, 26.2, 27.2, 28.2, 29.2]],])\n","\n","\n","  tensor_list = []\n","  tensor_list.append(test_tensor)\n","  tensor_list.append(test_tensor2)\n","  tensor_list.append(test_tensor3)\n","\n","  print(\"nearest\")\n","  print(\"Original Tensor: \", tensor_list)\n","  #custom_round(tensor_list, round_mode = \"nearest\", num_mantissa = 2)\n","  # fine_grain_custom_round(tensor_list, skip_processing = True,  \n","  #                         round_mode = \"exponent_reuse_new_fine_grain_dimension\", num_mantissa = 3, \n","  #                         block_size = 5, block_round_mode = \"afp_sector_update\")\n","  # print(\"Rounded Tensor: \", tensor_list)\n","\n","  fine_grain_custom_round(tensor_list, skip_processing = True,  \n","                          round_mode = \"exponent_reuse_new_fine_grain_dimension\", num_mantissa = 3, \n","                          block_size = 5, block_round_mode = \"afp_sector_update\")\n","  print(\"Rounded Tensor: \", tensor_list)\n","\n","\n"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"ldHUIctA4LGZ","executionInfo":{"status":"ok","timestamp":1628701415634,"user_tz":420,"elapsed":14,"user":{"displayName":"Thomas Yeh","photoUrl":"","userId":"16777654985296421037"}}},"source":["# Verification code for custom rounding\n","testing_rounding = False\n","\n","if (testing_rounding): \n","  \n","  test_tensor = tf.constant([ \n","    [[60, 1.5, 49.75, 0.25, 64],\n","    [5.25, 6.25, 7.25, 8.25, 8.45]],\n","    [[0.32, 0.22, 0.12, 0.52, 0.92],\n","    [0.52, 0.62, 0.72, 0.82, 0.2]],\n","    [[20.2, 21.2, 22.2, 23.2, 24.2],\n","    [25.2, 26.2, 27.2, 28.2, 29.2]],])\n","\n","  test_tensor2 = tf.constant([\n","    [[40, 1.5, 49.75, 0.25, 38],\n","    [5.5, 6.5, 7.5, 8.5, 8.9]],\n","    [[10.2, 11.2, 12.2, 13.2, 14.2],\n","    [15.2, 16.2, 17.2, 18.2, 19.2]],\n","    [[20.2, 21.2, 22.2, 23.2, 24.2],\n","    [25.2, 26.2, 27.2, 28.2, 29.2]],])\n","  \n","  test_tensor3 = tf.constant([\n","    [[40, 1.5, 49.75, 0.25, 38],\n","    [5.5, 6.5, 7.5, 8.5, 8.9]],\n","    [[1.27, -2.25, 4, 6.6, 8],\n","    [15.2, 16.2, 17.2, 18.2, 19.2]],\n","    [[20.2, 21.2, 22.2, 23.2, 24.2],\n","    [25.2, 26.2, 27.2, 28.2, 29.2]],])\n","\n","\n","  tensor_list = []\n","  tensor_list.append(test_tensor)\n","  tensor_list.append(test_tensor2)\n","  tensor_list.append(test_tensor3)\n","\n","  print(\"nearest\")\n","  print(\"Original Tensor: \", tensor_list)\n","  #custom_round(tensor_list, round_mode = \"nearest\", num_mantissa = 2)\n","  # fine_grain_custom_round(tensor_list, skip_processing = True,  \n","  #                         round_mode = \"exponent_reuse_new_fine_grain_dimension\", num_mantissa = 3, \n","  #                         block_size = 5, block_round_mode = \"afp_sector_update\")\n","  # print(\"Rounded Tensor: \", tensor_list)\n","\n","  fine_grain_custom_round(tensor_list, skip_processing = True,  \n","                          round_mode = \"exponent_reuse_new_fine_grain_dimension\", num_mantissa = 3, \n","                          block_size = 5, block_round_mode = \"afp_sector_update_jamming_1bit\")\n","  print(\"Rounded Tensor: \", tensor_list)\n","\n","\n"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"nLlCBcH23u2P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628701415635,"user_tz":420,"elapsed":15,"user":{"displayName":"Thomas Yeh","photoUrl":"","userId":"16777654985296421037"}},"outputId":"507a5c34-cc64-483a-9969-d9b3cd28716d"},"source":["# Verification code for custom rounding\n","# Need to debug jamming rounding - should not be rounding up.\n","\n","testing_rounding = True\n","\n","if (testing_rounding): \n","  \n","  test_tensor = tf.constant([ \n","    [[0, 1.5, 49.75, 0.25, 64],\n","    [5.63, 6.25, 7.25, 8.25, 8.45]],\n","    [[0.32, 0.22, 0.12, 0.52, 0.92],\n","    [0.52, 0.62, 0.72, 0.82, 0.2]],\n","    [[20.2, 21.2, 22.2, 23.2, 24.2],\n","    [25.2, 26.2, 27.2, 28.2, 29.2]],])\n","\n","  test_tensor2 = tf.constant([\n","    [[40.25, 1.5, 49.75, 0.125, 38],\n","    [5.5, 6.5, 7.5, 8.5, 8.9]],\n","    [[10.2, 11.2, 12.2, 13.2, 14.2],\n","    [15.2, 16.2, 17.2, 18.2, 19.2]],\n","    [[20.2, 21.2, 22.2, 23.2, 24.2],\n","    [25.2, 26.2, 27.2, 28.2, 29.2]],])\n","  \n","  test_tensor3 = tf.constant([\n","    [[40, 1.5, 49.75, 0.25, 38],\n","    [5.5, 6.5, 7.5, 8.5, 8.9]],\n","    [[1.27, -2.25, 4, 6.6, 8],\n","    [15.2, 16.2, 17.2, 18.2, 19.2]],\n","    [[20.2, 21.2, 22.2, 23.2, 24.2],\n","    [25.2, 26.2, 27.2, 28.2, 29.2]],])\n","\n","\n","  tensor_list = []\n","  tensor_list.append(test_tensor)\n","  tensor_list.append(test_tensor2)\n","  tensor_list.append(test_tensor3)\n","\n","  print(\"nearest\")\n","  print(\"Original Tensor: \", tensor_list)\n","  #custom_round(tensor_list, round_mode = \"nearest\", num_mantissa = 2)\n","  # fine_grain_custom_round(tensor_list, skip_processing = True,  \n","  #                         round_mode = \"exponent_reuse_new_fine_grain_dimension\", num_mantissa = 3, \n","  #                         block_size = 5, block_round_mode = \"afp_sector_update\")\n","  # print(\"Rounded Tensor: \", tensor_list)\n","\n","  fine_grain_custom_round(tensor_list, skip_processing = True,  \n","                          round_mode = \"exponent_reuse_new_fine_grain_dimension\", num_mantissa = 3, \n","                          block_size = 5, block_round_mode = \"afp_sector_update_jamming_zero_new\")\n","  print(\"Rounded Tensor: \", tensor_list)\n","\n","\n"],"execution_count":10,"outputs":[{"output_type":"stream","text":["nearest\n","Original Tensor:  [<tf.Tensor: shape=(3, 2, 5), dtype=float32, numpy=\n","array([[[ 0.  ,  1.5 , 49.75,  0.25, 64.  ],\n","        [ 5.63,  6.25,  7.25,  8.25,  8.45]],\n","\n","       [[ 0.32,  0.22,  0.12,  0.52,  0.92],\n","        [ 0.52,  0.62,  0.72,  0.82,  0.2 ]],\n","\n","       [[20.2 , 21.2 , 22.2 , 23.2 , 24.2 ],\n","        [25.2 , 26.2 , 27.2 , 28.2 , 29.2 ]]], dtype=float32)>, <tf.Tensor: shape=(3, 2, 5), dtype=float32, numpy=\n","array([[[40.25 ,  1.5  , 49.75 ,  0.125, 38.   ],\n","        [ 5.5  ,  6.5  ,  7.5  ,  8.5  ,  8.9  ]],\n","\n","       [[10.2  , 11.2  , 12.2  , 13.2  , 14.2  ],\n","        [15.2  , 16.2  , 17.2  , 18.2  , 19.2  ]],\n","\n","       [[20.2  , 21.2  , 22.2  , 23.2  , 24.2  ],\n","        [25.2  , 26.2  , 27.2  , 28.2  , 29.2  ]]], dtype=float32)>, <tf.Tensor: shape=(3, 2, 5), dtype=float32, numpy=\n","array([[[40.  ,  1.5 , 49.75,  0.25, 38.  ],\n","        [ 5.5 ,  6.5 ,  7.5 ,  8.5 ,  8.9 ]],\n","\n","       [[ 1.27, -2.25,  4.  ,  6.6 ,  8.  ],\n","        [15.2 , 16.2 , 17.2 , 18.2 , 19.2 ]],\n","\n","       [[20.2 , 21.2 , 22.2 , 23.2 , 24.2 ],\n","        [25.2 , 26.2 , 27.2 , 28.2 , 29.2 ]]], dtype=float32)>]\n","Rounded Tensor:  [<tf.Tensor: shape=(3, 2, 5), dtype=float32, numpy=\n","array([[[ 0.     ,  1.     , 50.     ,  1.     , 64.     ],\n","        [ 5.625  ,  6.25   ,  7.25   ,  8.25   ,  8.25   ]],\n","\n","       [[ 0.34375,  0.21875,  0.09375,  0.53125,  0.90625],\n","        [ 0.53125,  0.59375,  0.71875,  0.84375,  0.21875]],\n","\n","       [[20.5    , 21.5    , 22.5    , 23.5    , 24.5    ],\n","        [25.5    , 26.5    , 27.5    , 28.5    , 29.5    ]]],\n","      dtype=float32)>, <tf.Tensor: shape=(3, 2, 5), dtype=float32, numpy=\n","array([[[42.   ,  2.   , 50.   ,  0.   , 38.   ],\n","        [ 5.5  ,  6.5  ,  7.5  ,  8.5  ,  8.75 ]],\n","\n","       [[10.125, 11.125, 12.25 , 13.25 , 14.25 ],\n","        [15.5  , 16.5  , 17.5  , 18.5  , 19.5  ]],\n","\n","       [[20.5  , 21.5  , 22.5  , 23.5  , 24.5  ],\n","        [25.5  , 26.5  , 27.5  , 28.5  , 29.5  ]]], dtype=float32)>, <tf.Tensor: shape=(3, 2, 5), dtype=float32, numpy=\n","array([[[40.  ,  2.  , 50.  ,  2.  , 38.  ],\n","        [ 5.5 ,  6.5 ,  7.5 ,  8.5 ,  8.75]],\n","\n","       [[ 1.  , -3.  ,  4.  ,  6.  ,  8.  ],\n","        [15.5 , 16.5 , 17.5 , 18.5 , 19.5 ]],\n","\n","       [[20.5 , 21.5 , 22.5 , 23.5 , 24.5 ],\n","        [25.5 , 26.5 , 27.5 , 28.5 , 29.5 ]]], dtype=float32)>]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"79YMxVX3vCoC","executionInfo":{"status":"ok","timestamp":1628701415635,"user_tz":420,"elapsed":6,"user":{"displayName":"Thomas Yeh","photoUrl":"","userId":"16777654985296421037"}}},"source":["# sum(|rounded - original|)/ total_num -> average error\n","# sum(|rounded - original / original|) / total_sum -> average percentage error\n","from math import log\n","\n","def average_error(p, q):\n","  ae_average = 0\n","  ae_total = 0\n","  total_num = 0\n","\n","  for i, elem in enumerate(p):\n","    total_num += tf.size(elem).numpy()\n","    # zero_tensor = tf.zeros_like(elem)\n","    # boolean_tensor = tf.math.equal(elem, zero_tensor)\n","    x = tf.math.subtract(elem, q[i])\n","    x = tf.math.abs(x)\n","    x = tf.math.reduce_sum(x)\n","    ae_total += x.numpy()\n","  if total_num == 0:\n","    pass\n","  else:\n","    ae_average = ae_total / total_num\n","\n","  return ae_average, ae_total, total_num"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"pVJEAxKfvCoE","executionInfo":{"status":"ok","timestamp":1628701415636,"user_tz":420,"elapsed":6,"user":{"displayName":"Thomas Yeh","photoUrl":"","userId":"16777654985296421037"}}},"source":["from math import log\n","# sum(|rounded - original / original|) / total_sum -> average percentage error\n","def percent_average_error(p, q):\n","  ae_average = 0\n","  ae_total = 0\n","  total_num = 0\n","\n","  for i, elem in enumerate(p):\n","    total_num += tf.size(elem).numpy()\n","    # zero_tensor = tf.zeros_like(elem)\n","    # boolean_tensor = tf.math.equal(elem, zero_tensor)\n","    x = tf.math.subtract(elem, q[i])\n","    x = tf.math.divide(x, q[i])\n","    x = tf.where(tf.math.is_nan(x), tf.zeros_like(x), x)\n","    x = tf.math.abs(x)\n","    x = tf.math.reduce_sum(x)\n","    ae_total += x.numpy()\n","  if total_num == 0:\n","    pass\n","  else:\n","    ae_average = ae_total / total_num\n","\n","  return ae_average, ae_total, total_num"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"pimXKQWGfJGl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628715330422,"user_tz":420,"elapsed":5927000,"user":{"displayName":"Thomas Yeh","photoUrl":"","userId":"16777654985296421037"}},"outputId":"c3522fc5-0387-46bb-cd80-ff5a994feb76"},"source":["\n","\n","# This model uses the 2 different modes of AFP to deal with Batch Norm Layers\n","\n","########\n","#2ND RUN\n","########\n","import tensorflow as tf\n","import cv2\n","import numpy\n","###############\n","import copy\n","\n","#ds = ds.shuffle(1024).batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n","round_types = [\"exponent_reuse_new_fine_grain_dimension\"]\n","block_round_types = [\"afp_sector_update_jamming_zero_new\"]\n","num_man = 23\n","test_images = []\n","labels = []\n","acc = tf.keras.metrics.CategoricalAccuracy()\n","\n","block_size_list = [16]\n","\n","min_param_size = 10000\n","\n","first_layers = []\n","second_layers = []\n","\n","skip_layers = True\n","\n","#sample = ds.take(100)\n","\n","round_mode = \"exponent_reuse_new_fine_grain_dimension\"\n","\n","for block_round_mode in block_round_types:\n","\n","  for i in range(len(block_size_list)):\n","\n","    block_size = block_size_list[i]\n","    if (round_mode != \"exponent_reuse_new_fine_grain_dimension\" and i > 0):\n","      continue\n","\n","    for num_mantissa in range(4, 2, -1):\n"," \n","\n","      acc.reset_states()\n","      model = EfficientNetB0(weights='imagenet')\n","\n","      # 35 out of 49\n","      ####################\n","      # Initializing accuracy metric for per layer\n","      # 16 correct out of 20\n","      # data = [number_correct, total_predictions]\n","      # initialize to data = [0.0, 0.0]\n","      data = [0.0, 0.0]\n","      image_num = int(data[1])\n","      data[0] = tf.cast(data[0], tf.float32)\n","      data[1] = tf.cast(data[1], tf.float32)\n","      acc.set_weights(data)\n","      #print(\"Image: \", image_num, \" Accuracy: \", (acc.result().numpy()*100))\n","      ds = ds.skip(image_num)\n","      ####################      \n","\n","      rounded_layers = []\n","      for layer_elem in model.layers:\n","        #print(\"layer name: \", layer_elem.name)\n","        # if layer_elem.count_params() < min_param_size: \n","        #   continue \n","        # rounded_layers.append(layer_elem.name)\n","\n","        if ((skip_layers == False or \"conv\" in layer_elem.name) and (skip_layers == False or \"bn\" not in layer_elem.name)):\n","          #print(\"layer_num: \", layer_num)\n","          layer_weights = layer_elem.get_weights()\n","          #print(\"layer shape: \", tf.shape(layer_weights))\n","          if (round_mode == \"exponent_reuse_new_fine_grain_dimension\"):\n","            fine_grain_custom_round(layer_weights, round_mode = round_mode, skip_processing = True, \n","                                        block_round_mode = block_round_mode, block_size = block_size, \n","                                        num_mantissa=num_mantissa, is_tensor = False)\n","          else:\n","            custom_round(layer_weights, round_mode, num_mantissa=num_mantissa, is_tensor = False)\n","          layer_elem.set_weights(layer_weights)\n","        '''\n","        elif (skip_layers == True and \"bn\" in layer_elem.name):\n","          \n","          layer_weights = layer_elem.get_weights()\n","\n","          if (round_mode == \"exponent_reuse_new_fine_grain_dimension\"):\n","            fine_grain_custom_round(layer_weights, round_mode = round_mode, skip_processing = True, \n","                                    block_round_mode = \"afp_optimal_range_msfp_nearest_v2\", block_size = block_size, \n","                                    num_mantissa=num_mantissa, is_tensor = False)\n","          else:\n","            custom_round(layer_weights, round_mode = \"afp_optimal_range_msfp_nearest_v2\", num_mantissa=num_mantissa, is_tensor = False)\n","        '''  \n","\n","      # 100 samples for faster testing\n","  #    for example in ds.take(100):\n","  #    for example in ds:\n","      #image_num = 0\n","      #image_num = 0\n","      for example in ds:\n","        image_num += 1\n","\n","        image_elem, label_elem = example[\"image\"], example[\"label\"]\n","        test_image = image.img_to_array(image_elem)\n","\n","        # Resize\n","\n","        height, width, _ = test_image.shape\n","        new_height = height * 256 // min(test_image.shape[:2])\n","        new_width = width * 256 // min(test_image.shape[:2])\n","        test_image = cv2.resize(test_image, (new_width, new_height), interpolation=cv2.INTER_CUBIC)\n","\n","        # Crop\n","        height, width, _ = test_image.shape\n","        startx = width//2 - (224//2)\n","        starty = height//2 - (224//2)\n","        test_image = test_image[starty:starty+224,startx:startx+224]\n","        assert test_image.shape[0] == 224 and test_image.shape[1] == 224, (test_image.shape, height, width)\n","       \n","        test_image = np.expand_dims(test_image, axis=0)\n","        test_image = preprocess_input(test_image)\n","\n","        accumulate_total = 0\n","        accumulate_num = 0\n","\n","        paccumulate_total = 0\n","        paccumulate_num = 0         \n","\n","        output_dict = {}\n","        first = True\n","        for layer in model.layers:\n","          if first: #for the first layer case\n","            x = layer(test_image)\n","            prev_x = copy.deepcopy(x)  \n","\n","            # Apply rounding on x\n","            '''\n","            if layer.name in rounded_layers:\n","              if ((skip_layers == False or \"conv\" in layer.name) and (skip_layers == False or \"bn\" not in layer.name)):\n","                if (round_mode == \"exponent_reuse_new_fine_grain_dimension\"):\n","                  #print(x)\n","                  x = layer_outputs_custom_round(x, round_mode = round_mode, skip_processing = True, \n","                                          block_round_mode = block_round_mode, block_size = block_size, \n","                                          num_mantissa=num_mantissa, is_tensor = False)\n","                else:\n","                  custom_round(x, round_mode, num_mantissa=num_mantissa, is_tensor = False)\n","            '''\n","            output_dict[layer.name] = x\n","            first = False\n","          else: #for single input cases\n","            if not isinstance(layer.input, list):\n","              input_name = layer.input.name.split('/')[0]\n","              input = output_dict.get(input_name)\n","              x = layer(input)\n","              prev_x = copy.deepcopy(x)  \n","              # Apply rounding on x\n","\n","              '''\n","              if layer.name in rounded_layers:\n","                if ((skip_layers == False or \"conv\" in layer.name) and (skip_layers == False or \"bn\" not in layer.name)):\n","                  if (round_mode == \"exponent_reuse_new_fine_grain_dimension\"):\n","                    #print(x)\n","                    x = layer_outputs_custom_round(x, round_mode = round_mode, skip_processing = True, \n","                                            block_round_mode = block_round_mode, block_size = block_size, \n","                                            num_mantissa=num_mantissa, is_tensor = False)\n","                  else:\n","                    custom_round(x, round_mode, num_mantissa=num_mantissa, is_tensor = False)\n","              '''\n","\n","              layer_name = layer.name.split('/')[0]\n","              output_dict[layer_name] = x\n","            else: #for two input cases\n","              inputs = []\n","              for i, input_layer in enumerate(layer.input):\n","                input_name = layer.input[i].name.split('/')[0]\n","                input = output_dict.get(input_name)\n","                inputs.append(input)\n","              x = layer(inputs)\n","              prev_x = copy.deepcopy(x)  \n","\n","              # Apply rounding on x\n","              '''\n","              if layer.name in rounded_layers:\n","                if ((skip_layers == False or \"conv\" in layer.name) and (skip_layers == False or \"bn\" not in layer.name)):\n","                  if (round_mode == \"exponent_reuse_new_fine_grain_dimension\"):\n","                    #print(x)\n","                    x = layer_outputs_custom_round(x, round_mode = round_mode, skip_processing = True, \n","                                            block_round_mode = block_round_mode, block_size = block_size, \n","                                            num_mantissa=num_mantissa, is_tensor = False)\n","                  else:\n","                    custom_round(x, round_mode, num_mantissa=num_mantissa, is_tensor = False)\n","              '''\n","\n","              layer_name = layer.name.split('/')[0]\n","              output_dict[layer_name] = x\n","\n","          #avr_error, num, total = average_error(x, prev_x)\n","          #pavr_error, pnum, ptotal = percent_average_error(x, prev_x)\n","\n","          #accumulate_num += num\n","          #accumulate_total += total\n","          #paccumulate_num += pnum\n","          #paccumulate_total += ptotal \n","\n","        # if accumulate_num == 0:\n","        #   avg_total = 0\n","        # else:\n","        #   avg_total = accumulate_num / accumulate_total\n","        # if paccumulate_num == 0:\n","        #   pavg_total = 0\n","        # else:\n","        #   pavg_total = paccumulate_num / paccumulate_total \n","\n","        preds = x[0]\n","        label = tf.one_hot(label_elem, 1000)\n","        acc.update_state(label, preds)\n","        # per layer update\n","        raw_data = acc.get_weights()\n","        if (image_num % 1000 == 0):\n","          print(\"Image: \", image_num, \" Accuracy: \", (acc.result().numpy()*100), \"Raw Data: \", raw_data) #, \"Error: \", avg_total, \"Percent_Error: \", pavg_total)\n"," \n","      if (round_mode == \"exponent_reuse_new_fine_grain_dimension\"):\n","        print(\n","          f'DMG Round_mode: {block_round_mode}, Block_size: {block_size}, Num_mantissa: {num_mantissa}, '\n","          f'Accuracy: {(acc.result().numpy()*100)} '\n","        )\n","      else:\n","        print(\n","              f'DMG Round_mode: {round_mode}, Num_mantissa: {num_mantissa}, '\n","              f'Accuracy: {(acc.result().numpy()*100)} '\n","            )\n","  # decode the results into a list of tuples (class, description, probability)\n","  # (one such list for each sample in the batch)\n","# print(round_mode + \" Num Mantissa: \", num_man, ' Predicted:', decode_predictions(preds, top=3)[0])\n","\n","# Adding code to calculate Top-1 accuracy:\n","# Info from here: https://github.com/calebrob6/imagenet_validation/blob/master/2.%20Benchmark%20Keras%20pretrained%20models%20on%20ImageNet.ipynb"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0.h5\n","21839872/21834768 [==============================] - 0s 0us/step\n","Image:  1000  Accuracy:  50.599998235702515 Raw Data:  [506.0, 1000.0]\n","Image:  2000  Accuracy:  50.999999046325684 Raw Data:  [1020.0, 2000.0]\n","Image:  3000  Accuracy:  51.30000114440918 Raw Data:  [1539.0, 3000.0]\n","Image:  4000  Accuracy:  51.72500014305115 Raw Data:  [2069.0, 4000.0]\n","Image:  5000  Accuracy:  51.38000249862671 Raw Data:  [2569.0, 5000.0]\n","Image:  6000  Accuracy:  51.249998807907104 Raw Data:  [3075.0, 6000.0]\n","Image:  7000  Accuracy:  51.61428451538086 Raw Data:  [3613.0, 7000.0]\n","Image:  8000  Accuracy:  51.73749923706055 Raw Data:  [4139.0, 8000.0]\n","Image:  9000  Accuracy:  51.92221999168396 Raw Data:  [4673.0, 9000.0]\n","Image:  10000  Accuracy:  51.70999765396118 Raw Data:  [5171.0, 10000.0]\n","DMG Round_mode: afp_sector_update_jamming_zero_new, Block_size: 16, Num_mantissa: 4, Accuracy: 51.70999765396118 \n","Image:  1000  Accuracy:  6.400000303983688 Raw Data:  [64.0, 1000.0]\n","Image:  2000  Accuracy:  6.800000369548798 Raw Data:  [136.0, 2000.0]\n","Image:  3000  Accuracy:  6.499999761581421 Raw Data:  [195.0, 3000.0]\n","Image:  4000  Accuracy:  6.549999862909317 Raw Data:  [262.0, 4000.0]\n","Image:  5000  Accuracy:  6.81999996304512 Raw Data:  [341.0, 5000.0]\n","Image:  6000  Accuracy:  6.866666674613953 Raw Data:  [412.0, 6000.0]\n","Image:  7000  Accuracy:  6.65714293718338 Raw Data:  [466.0, 7000.0]\n","Image:  8000  Accuracy:  6.662499904632568 Raw Data:  [533.0, 8000.0]\n","Image:  9000  Accuracy:  6.599999964237213 Raw Data:  [594.0, 9000.0]\n","Image:  10000  Accuracy:  6.7100003361701965 Raw Data:  [671.0, 10000.0]\n","DMG Round_mode: afp_sector_update_jamming_zero_new, Block_size: 16, Num_mantissa: 3, Accuracy: 6.7100003361701965 \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hpirVW-OiJVF"},"source":["\n","DMG Round_mode: afp_sector_update_jamming_zero_new, Block_size: 16, Num_mantissa: 6, Accuracy: 62.199997901916504 \n","\n","DMG Round_mode: afp_sector_update_jamming_zero_new, Block_size: 16, Num_mantissa: 5, Accuracy: 61.729997396469116 "]}]}